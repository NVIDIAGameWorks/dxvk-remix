/*
* Copyright (c) 2023-2024, NVIDIA CORPORATION. All rights reserved.
*
* Permission is hereby granted, free of charge, to any person obtaining a
* copy of this software and associated documentation files (the "Software"),
* to deal in the Software without restriction, including without limitation
* the rights to use, copy, modify, merge, publish, distribute, sublicense,
* and/or sell copies of the Software, and to permit persons to whom the
* Software is furnished to do so, subject to the following conditions:
*
* The above copyright notice and this permission notice shall be included in
* all copies or substantial portions of the Software.
*
* THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
* IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
* FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
* THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
* LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
* FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
* DEALINGS IN THE SOFTWARE.
*/
#pragma once

#include "rtx/utility/common.slangh"
#include "rtx/utility/math.slangh"
#include "rtx/utility/packing.slangh"
#include "rtx/concept/ray/ray.h"
#include "rtx/pass/common_bindings.slangh"
#include "rtx/utility/debug_view_indices.h"

// Surface Interaction Constants

static const uint kFootprintFromRayOrigin = 0u;
static const uint kFootprintFromRayDirection = 1u;
static const uint kFootprintFromTextureCoordDiff = 2u;
static const uint kFootprintFromRayOriginClamped = 3u;

static const bool SurfaceGenerateTangents = true;
static const bool SurfaceIgnoreTangents = false;

// Surface Interaction Helper Functions

// Texture Gradient from Ray Cone Helper
// [2021, "Improved Shader and Texture Level of Detail Using Ray Cones"] (Listing 1)
// Computes the texture gradients for anisotropic texture filtering based on the intersection of a ray cone with a triangle.
// Approximates the ray cone as a cylinder based on the cone radius at the point of intersection and computes the axes of an
// ellipse formed by this cylinder's intersection with the triangle to derive the gradients from. Arguments:
// - intersectionPoint: The world space intersection point of the ray on the triangle.
// - triangleNormal: The world space normal of the triangle.
// - twoTriangleArea: Two times the world space area of the triangle (magnitude of a cross product spanning the triangle).
// - rayConeDir: The direction of the ray cone.
// - rayConeRadiusAtIntersection: The radius of the ray cone at the intersection point, typically computed as coneRadiusAtIntersection = hitT * tan(rayConeAngle) but
//   this may be approximated as coneRadiusAtIntersection = hitT * rayConeAngle when the ray cone angle is very close to 0.
// - positions: An array of positions of the triangle vertices in world space.
// - txcoords: An array of texture coordinates of the vertices of the triangle.
// - interpolatedTexCoordsAtIntersection: The interpolated texture coordinates at the intersection point.
// - texGradientX: The output for the first gradient of the texture coordinates.
// - texGradientY: The output for the second gradient of the texture coordinates.
void computeAnisotropicEllipseAxes(
  vec3 intersectionPoint, f16vec3 triangleNormal, float twoTriangleArea, f16vec3 rayConeDir,
  float16_t rayConeRadiusAtIntersection, vec3 positions[3], vec2 txcoords[3], vec2 interpolatedTexCoordsAtIntersection,
  inout vec2 texGradientX, inout vec2 texGradientY)
{
  // Compute ellipse axes.
  vec3 ellipseAxis0 = rayConeDir - dot(vec3(triangleNormal), vec3(rayConeDir)) * vec3(triangleNormal); // Project the ray direction onto the plane to find the ellipse axis.
  const vec3 rayDirPlaneProjection0 = ellipseAxis0 - dot(vec3(rayConeDir), ellipseAxis0) * vec3(rayConeDir); // Project the ellipse axis onto the plane defined by the ray cone's direction.
  ellipseAxis0 *= rayConeRadiusAtIntersection / max(0.0001, length(rayDirPlaneProjection0)); // Use similar triangles to find the scale of the axis.

  vec3 ellipseAxis1 = cross(vec3(triangleNormal), ellipseAxis0); // Find the other orthogonal ellipse axis via a cross product with the triangle's normal.
  const vec3 rayDirPlaneProjection1 = ellipseAxis1 - dot(vec3(rayConeDir), ellipseAxis1) * vec3(rayConeDir);
  ellipseAxis1 *= rayConeRadiusAtIntersection / max(0.0001, length(rayDirPlaneProjection1));

  // Compute barycentrics and texture coordinate gradients.
  const vec3 d = intersectionPoint - positions[0];
  const vec3 edge01 = positions[1] - positions[0];
  const vec3 edge02 = positions[2] - positions[0];
  const float oneOverTwoAreaTriangle = 1.0f / twoTriangleArea;

  vec3 edgeP;

  edgeP = d + ellipseAxis0;
  const float u1 = (dot(vec3(triangleNormal), cross(edgeP, edge02)) * oneOverTwoAreaTriangle);
  const float v1 = (dot(vec3(triangleNormal), cross(edge01, edgeP)) * oneOverTwoAreaTriangle);
  texGradientX = (1.0 - u1 - v1) * txcoords[0] + u1 * txcoords[1] + v1 * txcoords[2] - interpolatedTexCoordsAtIntersection;

  edgeP = d + ellipseAxis1;
  const float u2 = (dot(vec3(triangleNormal), cross(edgeP, edge02)) * oneOverTwoAreaTriangle);
  const float v2 = (dot(vec3(triangleNormal), cross(edge01, edgeP)) * oneOverTwoAreaTriangle);
  texGradientY = (1.0 - u2 - v2) * txcoords[0] + u2 * txcoords[1] + v2 * txcoords[2] - interpolatedTexCoordsAtIntersection;
}

// Surface Interaction Functions

bool isSurfaceClipped(Surface surface, MinimalSurfaceInteraction surfaceInteraction)
{
  if (!surface.isClipPlaneEnabled)
  {
    return false;
  }

  const float clipDistance = dot(surface.clipPlane.xyz, surfaceInteraction.position) + surface.clipPlane.w;

  return clipDistance < 0.0;
}
 
// <0, 1> -> <-0.5, 0.5>
vec2 texcoordToOffsetFromTextureCenter(vec2 texcoord)
{
  return texcoord - 0.5;
}

// <-0.5, 0.5> -> <0, 1>
vec2 offsetFromTextureCenterToTexcoord(vec2 offset)
{
  return offset + 0.5;
}

// Converts from <0, 1> texture space of a first cascade into <0, 1> texture space of the Nth cascade
vec2 convert1stCascadeTexCoordToTargetCascadeLevel(float rcpTargetCascadeLevelScale, vec2 texcoord)
{
  return offsetFromTextureCenterToTexcoord(rcpTargetCascadeLevelScale * texcoordToOffsetFromTextureCenter(texcoord));
}

// Converts from <0, 1> texture space of a Nth cascade into <0, 1> of a combined tiled texture space <0, 1>
vec2 convert1stCascadeTexCoordToTiledTexCoord(uvec2 cascade2DIndex, float rcpTargetCascadeLevelScale, vec2 texcoord)
{
  return cb.terrainArgs.rcpCascadeMapSize * (cascade2DIndex + convert1stCascadeTexCoordToTargetCascadeLevel(rcpTargetCascadeLevelScale, texcoord));
}

// Converts from <0, 1> texture space of a first cascade into texture coords of Nth cascade of a combined tiled texture space <0, 1>
// where cascades are stored as tiles left to right, top to bottom
void convert1stCascadeTexCoordsToTargetCascadeLevel(uint targetCascadeLevel, inout vec2 interpolatedTexcoord, inout vec2 texcoords[3])
{
  ivec2 cascade2DIndex;
  cascade2DIndex.y = targetCascadeLevel * cb.terrainArgs.rcpCascadeMapSize.x;
  cascade2DIndex.x = targetCascadeLevel - cascade2DIndex.y * cb.terrainArgs.cascadeMapSize.x;

  const float baseCascadeScale = targetCascadeLevel == cb.terrainArgs.maxCascadeLevel ? cb.terrainArgs.lastCascadeScale : 1.f;
  const float rcpTargetCascadeLevelScale = 1 / (baseCascadeScale * pow(2, targetCascadeLevel));

  interpolatedTexcoord = convert1stCascadeTexCoordToTiledTexCoord(cascade2DIndex, rcpTargetCascadeLevelScale, interpolatedTexcoord);

  [unroll]
  for (uint i = 0; i < 3; i++)
  {
    texcoords[i] = convert1stCascadeTexCoordToTiledTexCoord(cascade2DIndex, rcpTargetCascadeLevelScale, texcoords[i]);
  }
}

// Adjusts a specified set of texture coordinates and gradients with respect to specified spritesheet information.
void calcSpriteSheetAdjustment(
  inout vec2 textureCoordinates,
  inout vec2 textureGradientX,
  inout vec2 textureGradientY,
  float timeSinceStartSeconds,
  uint8_t spriteSheetRows,
  uint8_t spriteSheetCols,
  uint8_t spriteSheetFPS)
{
  // Note: Fast path for most materials which do not actually use animated spritesheets.
  if (spriteSheetFPS == 0)
  {
    return;
  }
  vec2 uvSize, uvBias;
  calcSpritesheetValues(
    timeSinceStartSeconds,
    spriteSheetRows,
    spriteSheetCols,
    spriteSheetFPS,
    uvSize,
    uvBias);

  textureCoordinates = uvBias + frac(textureCoordinates) * uvSize;
  textureGradientX *= uvSize;
  textureGradientY *= uvSize;
}

void calcSpritesheetValues(
  float timeSinceStartSeconds,
  uint8_t spriteSheetRows,
  uint8_t spriteSheetCols,
  uint8_t spriteSheetFPS,
  inout vec2 uvSize,
  inout vec2 uvBias
)
{
  const uint numSprites = spriteSheetCols * spriteSheetRows;
  // Note: timeSinceStartSeconds is clamped to (2^24 - 1) / 1000 on the CPU side so this multiplication shouldn't overflow.
  const uint frame = (uint(timeSinceStartSeconds * spriteSheetFPS)) % numSprites;

  uvSize = vec2(1.0f / spriteSheetCols, 1.0f / spriteSheetRows);
  uvBias = vec2(frame % spriteSheetCols, frame / spriteSheetCols) * uvSize;
}

SurfaceInteraction surfaceInteractionCreate<let GenerateTangents : bool>(
  Surface surface, RayInteraction rayInteraction, Ray ray,
  bool usePreviousPositions = false, uint footprintMode = kFootprintFromRayDirection)
{
  // Compute vertex indicies

  uint idx[3];

  for (uint i = 0; i < 3; i++)
  {
    const uint indexIndex = rayInteraction.primitiveIndex * 3 + i + surface.firstIndex;

    if (surface.indexBufferIndex != BINDING_INDEX_INVALID)
    {
      const uint indexBufferIndex = surface.indexBufferIndex;

      if (surface.indexStride == 4)
      {
        idx[i] = BUFFER_ARRAY(indices32, indexBufferIndex, indexIndex);
      }
      else
      {
        idx[i] = BUFFER_ARRAY(indices, indexBufferIndex, indexIndex);
      }
    }
    else
    {
      idx[i] = indexIndex;
    }
  }

  // Retrieve vertex data

  vec3 positions[3];

  if (usePreviousPositions && !surface.isStatic && surface.previousPositionBufferIndex != BINDING_INDEX_INVALID)
  {
    for (uint i = 0; i < 3; i++)
    {
      const uint currentIndex = idx[i];

      const uint previousPositionBufferIndex = surface.previousPositionBufferIndex;
      const uint previousPositionElementIndex = (currentIndex * uint(surface.positionStride) + surface.positionOffset) / 4;

      positions[i].x = BUFFER_ARRAY(geometries, previousPositionBufferIndex, previousPositionElementIndex + 0);
      positions[i].y = BUFFER_ARRAY(geometries, previousPositionBufferIndex, previousPositionElementIndex + 1);
      positions[i].z = BUFFER_ARRAY(geometries, previousPositionBufferIndex, previousPositionElementIndex + 2);
    }
  }
  else
  {
    for (uint i = 0; i < 3; i++)
    {
      const uint currentIndex = idx[i];

      // Note: Position buffer is always required for now
      // if (surface.positionBufferIndex != BINDING_INDEX_INVALID)
      {
        const uint positionBufferIndex = surface.positionBufferIndex;
        const uint positionElementIndex = (currentIndex * uint(surface.positionStride) + surface.positionOffset) / 4;

        positions[i].x = BUFFER_ARRAY(geometries, positionBufferIndex, positionElementIndex + 0);
        positions[i].y = BUFFER_ARRAY(geometries, positionBufferIndex, positionElementIndex + 1);
        positions[i].z = BUFFER_ARRAY(geometries, positionBufferIndex, positionElementIndex + 2);
      }
    }
  }

  const vec3 bary = uintToBarycentrics(rayInteraction.barycentricCoordinates);

  SurfaceInteraction surfaceInteraction;

  // Calculate vertex position in world space
  // Note: World space positions needed to compute the texture gradients, a bit expensive but probably easier than transforming
  // the ray direction into object space and worrying about how to transform its cone width appropriately (scales could affect it).
  // Additionally, this allows us to calculate the triangle normal in world space without having to transform an object space normal
  // into world space with a normal matrix which makes up for some of the work this does. Unfortunately the interpolated normal still
  // requires usage of a normal matrix though. World position is now also needed for the position calculation in general as it is higher
  // precision to calculate it this way on the Surface Interaction than on the Ray Interaction.

  vec3 worldPositions[3];

  if (usePreviousPositions && !surface.isStatic)
  {
    worldPositions[0] = mul(surface.prevObjectToWorld, vec4(positions[0], 1.0f));
    worldPositions[1] = mul(surface.prevObjectToWorld, vec4(positions[1], 1.0f));
    worldPositions[2] = mul(surface.prevObjectToWorld, vec4(positions[2], 1.0f));
  }
  else
  {
    worldPositions[0] = mul(surface.objectToWorld, vec4(positions[0], 1.0f));
    worldPositions[1] = mul(surface.objectToWorld, vec4(positions[1], 1.0f));
    worldPositions[2] = mul(surface.objectToWorld, vec4(positions[2], 1.0f));
  }

  // Calculate the approximate position error in object space
  surfaceInteraction.positionError = max(
    calculatePositionError(positions[0]),
    max(calculatePositionError(positions[1]),
        calculatePositionError(positions[2])));

  // Compute world position
  // Note: This calculation is done with world space positions rather than model space positions and transforming it to world space
  // as we have these world space positions available to begin with as they are needed for other calculations. Additionally this way
  // is technically more precise as well.

  const vec3 hitPosition = interpolateHitAttribute(worldPositions, bary);

  surfaceInteraction.position = hitPosition;

  // Update the position error because it might be larger in world space than in object space
  surfaceInteraction.positionError = max(surfaceInteraction.positionError, calculatePositionError(hitPosition));

  // Compute various normals

  const vec3 ab = worldPositions[1] - worldPositions[0];
  const vec3 ac = worldPositions[2] - worldPositions[0];
  const vec3 worldTriangleVector = cross(ab, ac);

  // Note: Fall back to an arbitrary vector for degenerate triangles where a triangle normal cannot be derived. Note this
  // will not produce a valid normal for cases where the positional data itself is NaN/Inf, but these cases likely indicate
  // corruption in the original data to begin with.
  const vec3 worldTriangleNormal = safeNormalize(worldTriangleVector, vec3(0.0f, 0.0f, 1.0f));
  // Note: Ideally cheaper than using length to get the magnitude of the cross product's result (no sqrt required).
  const float worldTwoTriangleArea = dot(worldTriangleVector, worldTriangleNormal);
  surfaceInteraction.triangleArea = worldTwoTriangleArea * 0.5;

  // Note: fp16 conversion done after area calculation so the fp32 normal can be used
  const f16vec3 triangleNormal = f16vec3(worldTriangleNormal);
  const bool triangleNormalSameDirection = dot(triangleNormal, rayInteraction.viewDirection) >= float16_t(0.0);
  const f16vec3 flippedTriangleNormal = triangleNormalSameDirection ? triangleNormal : -triangleNormal;

  surfaceInteraction.triangleNormal = flippedTriangleNormal;

  if (surface.normalBufferIndex == BINDING_INDEX_INVALID)
  {
    surfaceInteraction.interpolatedNormal = surfaceInteraction.triangleNormal;
  }
  else
  {
    vec3 normals[3];
  
    for (uint i = 0; i < 3; i++)
    {
      const uint currentIndex = idx[i];
  
      const uint normalBufferIndex = surface.normalBufferIndex;
      const uint normalElementIndex = (currentIndex * uint(surface.normalStride) + surface.normalOffset) / 4;
  
      if (surface.normalsEncoded)
      {
        const uint encodedNormal = floatBitsToUint(BUFFER_ARRAY(geometries, normalBufferIndex, normalElementIndex));
        const vec3 decodedNormal = unsignedOctahedralToSphereDirection(vec2(
          unorm16ToF32(uint16_t(encodedNormal >> 0)),
          unorm16ToF32(uint16_t(encodedNormal >> 16))
        ));

        normals[i] = decodedNormal;
      }
      else
      {
        normals[i].x = BUFFER_ARRAY(geometries, normalBufferIndex, normalElementIndex + 0);
        normals[i].y = BUFFER_ARRAY(geometries, normalBufferIndex, normalElementIndex + 1);
        normals[i].z = BUFFER_ARRAY(geometries, normalBufferIndex, normalElementIndex + 2);
      }
    }
  
    // Note: Fall back to the triangle normal when an interpolated normal cannot be derived. Note this will not produce a
    // valid normal for cases where the positional data itself is NaN/Inf, but these cases likely indicate corruption in
    // the original data to begin with.
    const vec3 objectInterpolatedNormal = safeNormalize(interpolateHitAttribute(normals, bary), triangleNormal);
  
    // Note: Second normalization required as vector may be scaled through a normal transformation
    const f16vec3 interpolatedNormal = f16vec3(normalize(mul(surface.normalObjectToWorld, objectInterpolatedNormal)));
    // Really the only reason to flip this normal is for double-sided geometry.  Do not flip based on view vector!
    const bool interpolatedNormalSameDirection = dot(interpolatedNormal, flippedTriangleNormal) >= float16_t(0.0);
    const f16vec3 flippedInterpolatedNormal = interpolatedNormalSameDirection ? interpolatedNormal : -interpolatedNormal;

    surfaceInteraction.interpolatedNormal = getBentNormal(surfaceInteraction.triangleNormal, flippedInterpolatedNormal, ray.direction);
  }

  // Compute motion
  // Note: Ignore the motion if the caller needs previous positions, we don't know it.

  if (!surface.isStatic && !usePreviousPositions)
  {
    vec3 prevObjectPosition = interpolateHitAttribute(positions, bary);

    if (surface.previousPositionBufferIndex != BINDING_INDEX_INVALID)
    {
      vec3 prevPositions[3];

      for (uint i = 0; i < 3; i++)
      {
        const uint currentIndex = idx[i];

        const uint previousPositionBufferIndex = surface.previousPositionBufferIndex;
        const uint previousPositionElementIndex = (currentIndex * uint(surface.positionStride) + surface.positionOffset) / 4;

        prevPositions[i].x = BUFFER_ARRAY(geometries, previousPositionBufferIndex, previousPositionElementIndex + 0);
        prevPositions[i].y = BUFFER_ARRAY(geometries, previousPositionBufferIndex, previousPositionElementIndex + 1);
        prevPositions[i].z = BUFFER_ARRAY(geometries, previousPositionBufferIndex, previousPositionElementIndex + 2);
      }

      prevObjectPosition = interpolateHitAttribute(prevPositions, bary);
    }

    const vec3 prevPosition = mul(surface.prevObjectToWorld, vec4(prevObjectPosition, 1.0f));

    surfaceInteraction.motion = prevPosition - hitPosition;
  }
  else
  {
    surfaceInteraction.motion = vec3(0.0);
  }

  // Compute texture coordinates

  if (surface.texcoordBufferIndex != BINDING_INDEX_INVALID || surface.texcoordGenerationMode != uint(TexGenMode::None))
  {
    // Note: This can be potentially extended to vec3 or vec4 if we ever want to support more than 2 texture transform elements
    // (potentially useful for say cubemap sampling, but Remix has little use to actually render cubemaps properly outside
    // of say sky capturing since ray traced reflections will replace most cubemap effects).
    vec2 texcoords[3];

    for (uint i = 0; i < 3; i++)
    {
      vec4 rawTextureCoordinate;

      if (surface.texcoordGenerationMode == uint(TexGenMode::ViewPositions) ||
          // For now, calculate texcoords for the first cascade. The texcoords for an appropriate cascade will get adjusted based on the interpolated texcoord later
          surface.texcoordGenerationMode == uint(TexGenMode::CascadedViewPositions))
      {
        const float4 viewPos = mul(cb.camera.worldToView, float4(worldPositions[i], 1.f));

        rawTextureCoordinate = viewPos;
      }
      else if (surface.texcoordGenerationMode == uint(TexGenMode::ViewNormals)) 
      {
        const uint currentIndex = idx[i];
        const uint normalBufferIndex = surface.normalBufferIndex;
        const uint normalElementIndex = (currentIndex * uint(surface.normalStride) + surface.normalOffset) / 4;

        float3 normal;
        normal.x = BUFFER_ARRAY(geometries, normalBufferIndex, normalElementIndex + 0);
        normal.y = BUFFER_ARRAY(geometries, normalBufferIndex, normalElementIndex + 1);
        normal.z = BUFFER_ARRAY(geometries, normalBufferIndex, normalElementIndex + 2);

        normal = normalize(mul(surface.normalObjectToWorld, normal));

        const float4 viewPos = mul(cb.camera.worldToView, float4(normal, 0.f));

        // Todo: Use the full vec4 viewPos here if rawTextureCoordinate is ever expanded to be a vec4 (right
        // now the other 2 components will never matter in our current implementation).
        rawTextureCoordinate = viewPos;
      }
      else
      {
        const uint currentIndex = idx[i];
        uint texcoordBufferIndex = surface.texcoordBufferIndex;
        const uint texcoordElementIndex = (currentIndex * uint(surface.texcoordStride) + surface.texcoordOffset) / 4;

        const vec2 loadedTextureCoordinate = vec2(
          BUFFER_ARRAY(geometries, texcoordBufferIndex, texcoordElementIndex + 0),
          BUFFER_ARRAY(geometries, texcoordBufferIndex, texcoordElementIndex + 1)
        );

        // Todo: This will likely need to load more than a vec2 of texture coordinates too if we ever support anything more complex.
        // A value of 1 is added to every component beyond what is loaded, though it really should be a value of 0 up to the texture
        // transform element count and 1 past that I think, check DXVK's d3d9_fixed_function.cpp for more info.
        rawTextureCoordinate = float4(loadedTextureCoordinate, 1.0f, 1.0f);
      }

      const vec2 transformedTextureCoordinate = mul(surface.textureTransform, rawTextureCoordinate);

      // Todo: Perspective division here on the proper component (as specified by the number of elements the transform is expected to output)
      // when projective texture transforms are enabled.

      // Todo: Use the full vec4 here if transformedTextureCoordinate is ever expanded to be a vec4 (right
      // now the other 2 components will never matter in our current implementation, and this covers the
      // 1 component case as sampling from a 1D texture will just ignore the second coordinate, though not sure
      // what would happen with a 2D texture given only a 1D transform result...).
      texcoords[i] = transformedTextureCoordinate;
    }

    surfaceInteraction.textureCoordinates = interpolateHitAttribute(texcoords, bary);

    if (GenerateTangents)
    {
      // Note: World positions used here, the original PBRT usage uses model space vertex positions since it calculates the
      // TBN  in local space, but since we generate the TBN frame in world space this is more applicable. Additionally
      // it removes the need to keep the object space positions around which should ideally reduce register pressure.
      genTangSpace(
      texcoords, worldPositions, surfaceInteraction.interpolatedNormal,
      surfaceInteraction.interpolatedTangent, surfaceInteraction.interpolatedBitangent, surfaceInteraction.rawTangent, surfaceInteraction.rawBitangent);
    
      // Derive triangle TBN based on interpolated TBN. This works in all cases tested,
      // and should be much cheaper than calling genTangSpace again.
      f16vec4 interpolatedToTriangle = quaternionCreateOrientation(surfaceInteraction.interpolatedNormal, surfaceInteraction.triangleNormal);
      surfaceInteraction.triangleTangent = quaternionTransformVector(interpolatedToTriangle, surfaceInteraction.interpolatedTangent);
      surfaceInteraction.triangleBitangent = quaternionTransformVector(interpolatedToTriangle, surfaceInteraction.interpolatedBitangent);
    }
    
    // Convert texcoords from the 1st cascade to a corresponding cascade in a combined tiled cascades texture space
    if (surface.texcoordGenerationMode == uint(TexGenMode::CascadedViewPositions))
    {
      uint cascadeLevel = 0;

      // Calculate the texcoords for cascade maps with more than one cascade.
      // Texcoords for the first cascade are already correct if there's only a single cascade in the map
      if (cb.terrainArgs.maxCascadeLevel > 0) 
      {
        // For surfaces with displacement, we need to ensure the final hit point is within a cascade level.
        // To do this, calculate the furthest texcoord that displacement could return, and use a cascade level that includes that point.
        const f16mat3 worldToTangent = f16mat3(surfaceInteraction.interpolatedTangent, surfaceInteraction.interpolatedBitangent, surfaceInteraction.interpolatedNormal);
        const f16vec3 viewDirTangentSpace = normalize(mul(worldToTangent, rayInteraction.viewDirection));
        // viewDirection is hitPos to camera, so invert it before converting to a texcoord offset.
        const vec2 maxPossiblePomOffset = viewDirTangentSpace.xy * (-1.f * cb.terrainArgs.displaceIn / viewDirTangentSpace.z);

        const vec2 textureCenterOffset = texcoordToOffsetFromTextureCenter(surfaceInteraction.textureCoordinates + maxPossiblePomOffset);
        const float maxTextureCenterOffset = length(textureCenterOffset);

        // Calculate a cascade level if sampling outside the first cascade
        if (maxTextureCenterOffset >= 0.5)
        {
          // Convert the offset from <0, 0.5> to <0, 1> of the first cascade 
          const float normalizedTextureCenterOffset = maxTextureCenterOffset * 2;

          // Calculate the cascade level starting from cascade level 1
          cascadeLevel = min(ceil(log2(normalizedTextureCenterOffset)), cb.terrainArgs.maxCascadeLevel);
        }
        // Note: This is safe to do after the Tangent space generation because it only translates and scales.  It does not change the normalized derivatives.
        convert1stCascadeTexCoordsToTargetCascadeLevel(cascadeLevel, surfaceInteraction.textureCoordinates, texcoords);
      }

#if defined(RAY_TRACING_PRIMARY_RAY)
      if (cb.debugView == DEBUG_VIEW_CASCADE_LEVEL)
      {
        storeInDebugView(getDispatchRaysIndex().xy, cascadeLevel);
      }
      else if (cb.debugView == DEBUG_VIEW_IS_BAKED_TERRAIN)
      {
        storeInDebugView(getDispatchRaysIndex().xy, 1);
      }
#endif
    }
    
    // Compute texture gradients
    // Note: This is done here as it depends on the triangle normal and texture coordinates being calculated first

    // Note: Non-flipped world space triangle normal passed in to be consistent with the calculations done internally (rather
    // than worrying about if the flipped vector could break it)
    if (footprintMode != kFootprintFromTextureCoordDiff)
    {
      const vec3 positionOffset = surfaceInteraction.position - ray.origin;
      const f16vec3 direction = footprintMode == kFootprintFromRayDirection ? ray.direction : normalize(positionOffset);
      const float16_t coneRadius = footprintMode == kFootprintFromRayDirection ? rayInteraction.coneRadius : rayInteraction.coneRadius + length(positionOffset) * ray.spreadAngle;

      computeAnisotropicEllipseAxes(
        hitPosition, triangleNormal, worldTwoTriangleArea, direction,
        coneRadius, worldPositions, texcoords, surfaceInteraction.textureCoordinates,
        surfaceInteraction.textureGradientX, surfaceInteraction.textureGradientY);

      if (footprintMode == kFootprintFromRayOriginClamped)
      {
        vec2 dUV1 = texcoords[1] - texcoords[0];
        vec2 dUV2 = texcoords[2] - texcoords[0];
        float dUVCross = abs(dUV1.x * dUV2.y - dUV2.x * dUV1.y);
        float radius = dUVCross / (length(dUV1) + length(dUV2) + length(dUV2 - dUV1));
        float maxFootprint = radius * 1.5;
        surfaceInteraction.textureGradientX *= min(1.0, maxFootprint / length(surfaceInteraction.textureGradientX));
        surfaceInteraction.textureGradientY *= min(1.0, maxFootprint / length(surfaceInteraction.textureGradientY));
      }
    }
    else
    {
      surfaceInteraction.textureGradientX = (texcoords[1] - texcoords[0]) / 3.0f;
      surfaceInteraction.textureGradientY = (texcoords[2] - texcoords[0]) / 3.0f;
    }
  }
  else
  {
    surfaceInteraction.textureGradientX = 0.f;
    surfaceInteraction.textureGradientY = 0.f;
    surfaceInteraction.textureCoordinates = vec2(0.f);

    // Create dummy tangent basis thats reasonable WRT normal
    calcOrthonormalBasis(
      surfaceInteraction.interpolatedNormal,
      surfaceInteraction.interpolatedTangent,
      surfaceInteraction.interpolatedBitangent);

    calcOrthonormalBasis(
      surfaceInteraction.triangleNormal,
      surfaceInteraction.triangleTangent,
      surfaceInteraction.triangleBitangent);
  }

  // Adjust texture coordinates and gradients for animated sprite sheet logic

  if (!surface.skipSurfaceInteractionSpritesheetAdjustment)
  {
    calcSpriteSheetAdjustment(
      surfaceInteraction.textureCoordinates,
      surfaceInteraction.textureGradientX,
      surfaceInteraction.textureGradientY,
      cb.timeSinceStartSeconds,
      surface.spriteSheetRows,
      surface.spriteSheetCols,
      surface.spriteSheetFPS);
  }

  // Baked terrain is the only surface using cascaded view positions, so key off of that. 
  const bool isBakedTerrain = surface.texcoordGenerationMode == uint(TexGenMode::CascadedViewPositions);

  // Compute vertex colors

  if (surface.color0BufferIndex != BINDING_INDEX_INVALID &&
     // Baked terrain has vertex colors already baked in, so default to neutral vertex color of 1.
     // They won't get reapplied later anyway
     !isBakedTerrain)
  {
    f16vec4 color[3];

    for (uint i = 0; i < 3; i++)
    {
      const uint currentIndex = idx[i];

      const uint color0BufferIndex = surface.color0BufferIndex;
      const uint color0ElementIndex = (currentIndex * uint(surface.color0Stride) + surface.color0Offset) / 4;

      // VK_FORMAT_B8G8R8A8_UNORM
      const uint colorBits = floatBitsToUint(BUFFER_ARRAY(geometries, color0BufferIndex, color0ElementIndex));

      color[i].x = unorm8ToF16(uint8_t(colorBits >> 16));
      color[i].y = unorm8ToF16(uint8_t(colorBits >> 8));
      color[i].z = unorm8ToF16(uint8_t(colorBits));
      color[i].w = unorm8ToF16(uint8_t(colorBits >> 24));
    }

    surfaceInteraction.vertexColor = interpolateHitAttribute(color, bary);
  }
  else
  {
    surfaceInteraction.vertexColor = f16vec4(1.0, 1.0, 1.0, 1.0);
  }

  return surfaceInteraction;
}

// Todo: Surface interaction construction function from sampled position

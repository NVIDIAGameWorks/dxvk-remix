/*
* Copyright (c) 2022-2025, NVIDIA CORPORATION. All rights reserved.
*
* Permission is hereby granted, free of charge, to any person obtaining a
* copy of this software and associated documentation files (the "Software"),
* to deal in the Software without restriction, including without limitation
* the rights to use, copy, modify, merge, publish, distribute, sublicense,
* and/or sell copies of the Software, and to permit persons to whom the
* Software is furnished to do so, subject to the following conditions:
*
* The above copyright notice and this permission notice shall be included in
* all copies or substantial portions of the Software.
*
* THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
* IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
* FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
* THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
* LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
* FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
* DEALINGS IN THE SOFTWARE.
*/
#pragma once

#include "rtx/algorithm/geometry_resolver_state.slangh"
#include "rtx/utility/noise.slangh"
#include "rtx/utility/procedural_noise.slangh"
#include "rtx/utility/packing.slangh"
#include "rtx/utility/froxel.slangh"
#include "rtx/utility/gbuffer_helpers.slangh"
#include "rtx/utility/debug_view_helpers.slangh"
#include "rtx/concept/camera/camera.slangh"
#include "rtx/concept/ray/ray.slangh"
#include "rtx/concept/surface/surface.slangh"
#include "rtx/concept/surface_material/surface_material.slangh"
#include "rtx/concept/surface_material/opaque_surface_material_interaction.slangh"
#include "rtx/concept/surface/alpha_blend_surface.slangh"
#include "rtx/concept/light/light.slangh"
#include "rtx/algorithm/resolve.slangh"
#include "rtx/algorithm/volume_lighting.slangh"
#include "rtx/pass/post_fx/post_fx_motion_blur_geometry_flags.slangh"
#include "rtx/algorithm/nee_cache_light.slangh"
#include "rtx/utility/gpu_printing.slangh"

// Geometry Resolver Helper Constants

static const uint8_t psrTypeReflection = uint8_t(0);
static const uint8_t psrTypeTransmission = uint8_t(1);

static const uint psrBufferReflection = 0;
static const uint psrBufferTransmission = 1;

// Geometry Resolver Helper Functions

GeometryResolverState geometryResolverStateCreateEmpty(u16vec2 pixelCoordinate)
{
  GeometryResolverState geometryResolverState;

  geometryResolverState.origin = vec3(0.0f, 0.0f, 0.0f);
  geometryResolverState.coneRadius = float16_t(0.0f);
  geometryResolverState.direction = f16vec3(0.0f, 0.0f, 0.0f);
  geometryResolverState.directionAltered = false;

  geometryResolverState.radiance = vec3(0.0f, 0.0f, 0.0f);
  geometryResolverState.attenuation = f16vec3(1.0f, 1.0f, 1.0f);
  geometryResolverState.segmentHitDistance = 0.0f;
  geometryResolverState.accumulatedHitDistance = 0.0f;
  geometryResolverState.rayMask = uint8_t(0x0);
  geometryResolverState.portalSpace = PORTAL_SPACE_NONE;
  geometryResolverState.isViewModelSurface = false;
  geometryResolverState.accumulatedRotation = getIdentityQuaternion();

  geometryResolverState.continueResolving = true;
  // Note: Inside medium assumed to start as false for the camera ray, this assumption will break down if the camera is ever intended
  // to work properly from within a translucent object (for example, underwater).
  geometryResolverState.insideMedium = false;
  geometryResolverState.performPSRR = false;
  geometryResolverState.performPSTR = false;
  geometryResolverState.reflectionSelectedIntegrationSurface = false;
  geometryResolverState.decalEncountered = false;
  geometryResolverState.useAlternateDisocclusionThreshold = false;
  geometryResolverState.pomOpaqueSurfaceEncountered = false;

  return geometryResolverState;
}

vec2 calcMotionVectorForRayMiss(const uvec2 pixelCoordinate)
{
   const float farPlane = cameraGetFarPlane(cb.camera);

   const Ray originalCameraRay = rayCreatePrimaryFromPixel(cb.camera, pixelCoordinate, false);
   const vec3 virtualHitPosition = rayEvaluate(originalCameraRay, farPlane);
   vec4 prevNDC = mul(cb.camera.prevWorldToProjection, vec4(virtualHitPosition, 1.0f));
   prevNDC /= prevNDC.w;

   vec2 ndc = cameraPixelCoordinateToNDC(cb.camera, pixelCoordinate);
   vec2 motionVectorNDC = prevNDC.xy - ndc.xy;
   vec2 motionVectorUVOffset = motionVectorNDC * vec2(0.5, -0.5);
   return motionVectorUVOffset * cb.camera.resolution;
}

// Geometry Resolver Vertex Functions

void geometryResolverOutputMiss(
  bool primarySurface, bool selectedIntegrationSurface, uvec2 pixelCoordinate)
{
  const float missHitDistance = -1.0f;

  if (primarySurface)
  {
    PrimaryHitDistance[pixelCoordinate] = missHitDistance;
    PrimaryScreenSpaceMotionVector[pixelCoordinate] = calcMotionVectorForRayMiss(pixelCoordinate);
    PrimarySurfaceFlags[pixelCoordinate] = 0;
    PrimaryDisocclusionThresholdMix[pixelCoordinate] = 0;
    PrimaryDepth[pixelCoordinate] = cb.clearColorDepth;

    // RR requires zeroed out input guide buffers for misses
    if (cb.enableDLSSRR)
    {
      PrimaryPerceptualRoughness[pixelCoordinate] = 0.f;
    }

    // Note: Always write to stochastic buffers in the primary surface case.
    writeMissToGBuffer(
      pixelCoordinate, true, cb.primaryDirectNrd.missLinearViewZ,
      PrimaryConeRadius, PrimaryLinearViewZ,
      PrimaryVirtualMotionVector,
      PrimaryVirtualWorldShadingNormalPerceptualRoughness,
      PrimaryVirtualWorldShadingNormalPerceptualRoughnessDenoising,
      PrimaryAlbedo, PrimaryBaseReflectivity, aliasedData0.PrimaryWorldPositionWorldTriangleNormal);

    if (cb.enableStochasticAlphaBlend)
    {
      AlphaBlendSurface alphaBlendSurface = AlphaBlendSurface.createFromPacked(AlphaBlendGBuffer[pixelCoordinate]);    
      alphaBlendSurface.finalize(0);
      AlphaBlendGBuffer[pixelCoordinate] = alphaBlendSurface.pack();
    }
  }
  else
  {
    SecondaryHitDistance[pixelCoordinate] = missHitDistance;

    // Note: Only write to stochastic buffers if this is the selected surface in the secondary surface case.
    writeMissToGBuffer(
      pixelCoordinate, selectedIntegrationSurface, cb.secondaryCombinedNrd.missLinearViewZ,
      SecondaryConeRadius, SecondaryLinearViewZ,
      SecondaryVirtualMotionVector,
      SecondaryVirtualWorldShadingNormalPerceptualRoughness,
      SecondaryVirtualWorldShadingNormalPerceptualRoughnessDenoising,
      SecondaryAlbedo, SecondaryBaseReflectivity, SecondaryWorldPositionWorldTriangleNormal);
  }
}

void geometryResolverOutputSurface(
  bool primarySurface, bool selectedIntegrationSurface, uvec2 pixelCoordinate,
  Ray ray, RayInteraction rayInteraction,
  Surface surface, SurfaceInteraction surfaceInteraction,
  PolymorphicSurfaceMaterialInteraction polymorphicSurfaceMaterialInteraction,
  vec3 radiance, f16vec3 attenuation, f16vec4 accumulatedRotation,
  uint16_t mediumMaterialIndex, float virtualHitDistance, bool directionAltered,
  bool useAlternateDisocclusionThreshold, bool isViewModel)
{
  // Output Attenuation (Primary/secondary surface coherent)

  const uint encodedAttenuation = colorToR11G11B10(attenuation);

  if (primarySurface)
  {
    imageStore(PrimaryAttenuation, ivec2(pixelCoordinate), encodedAttenuation);
  }
  else
  {
    imageStore(SecondaryAttenuation, ivec2(pixelCoordinate), encodedAttenuation);
  }

  // Recreate the original primary camera ray
  // Note: This is done to not have to pass extra information into the payload.

  const Ray originalCameraRay = rayCreatePrimaryFromPixel(cb.camera, pixelCoordinate, false);

  // Output Linear View Z (Primary/secondary surface coherent)

  // Note: Using virtual hit distance accumulated through PSR and Ray Portals used to calculate a virtual hit position
  const vec3 virtualHitPosition = rayEvaluate(originalCameraRay, virtualHitDistance);
  const float linearViewZ = cameraWorldPosToLinearViewZ(cb.camera, virtualHitPosition);

  if (primarySurface)
  {
    const float4 projectionSpacePosition = mul(cb.camera.worldToProjectionJittered, float4(virtualHitPosition, 1.f));
    // Note: This division will result in -infinity when the hit point is equal to the camera's position (z is negative assuming a non-zero near plane and w = 0).
    // This is "fine" though as in theory things should be able to handle this value properly (as it's not a NaN, though it could be if the near plane was 0). If
    // this causes problems in the future though, simply clamp this to some very negative value (e.g. negative float 16 or 32 max).
    const float depth = projectionSpacePosition.z / projectionSpacePosition.w;

    imageStore(PrimaryDepth, pixelCoordinate, depth);
    imageStore(PrimaryLinearViewZ, pixelCoordinate, linearViewZ);
    imageStore(PrimaryHitDistance, pixelCoordinate, virtualHitDistance);

    // Set value of 10000.f that forces RR to discard history.
    // Set the log value of 10000.f since RR uses log scale. RR prepass will revert the log operation after blurring the values.
    const float kDLSSRRDisocclusionThresholdValue = 
      cb.setLogValueForDisocclusionMaskForDLSSRR 
      ? 9.2103403719762 // == log(10000.f)
      : 10000.f;
    const float kDisocclusionThresholdValue = 
      cb.enableDLSSRR 
      ? kDLSSRRDisocclusionThresholdValue
      : 1.f;
    const bool discardRRHistory = surface.hasMaterialChanged || surface.isAnimatedWater;

    // Note: Fully use an alternate disocclusion threshold when specified, done by setting the mix value to 1.0 (this mixes
    // between the standard and alternate disocclusion threshold specified in the denoiser's settings).
    const float disocclusionThresholdMix = (discardRRHistory || useAlternateDisocclusionThreshold) ? kDisocclusionThresholdValue : 0.0f;

    // Note: May seem overkill to store a bool like this, but the denoiser expects an input unorm texture to lerp between the standard
    // and alternate disocclusion threshold so it must be expanded and stored to a texture like this at some point.
    imageStore(PrimaryDisocclusionThresholdMix, pixelCoordinate, disocclusionThresholdMix);
  }
  else
  {
    imageStore(SecondaryLinearViewZ, pixelCoordinate, linearViewZ);
    imageStore(SecondaryHitDistance, pixelCoordinate, virtualHitDistance);
  }

  // Output Motion Vector (Primary/secondary surface coherent)
  // Note: Technically part of the Surface Interaction, just not something we'd want to deserialize in other passes which would mean we'd need yet
  // another structure for this. To avoid that, just output it like this for now.

  // Note: Virtual motion used to account for the ray traversing ray portals and through PSR interactions.
  const vec3 virtualMotion = quaternionTransformVector(accumulatedRotation, surfaceInteraction.motion, true);
  
  if (primarySurface)
  {
    imageStore(PrimaryVirtualMotionVector, ivec2(pixelCoordinate), vec4(virtualMotion, 0));

    const vec4 prevWorldPosition = vec4(virtualHitPosition + virtualMotion, 1);
    const vec4 prevClip = mul(cb.camera.prevWorldToProjection, prevWorldPosition);
    vec2 prevNDC;

    if (prevClip.w == 0.0f) {
      // Note: This is a bit of an odd case, when w = 0 the point is the same as the camera's position, and this is represented by a Z value of
      // -infinity (as Z values are negative when behind the near plane at Z = 0 before wrapping to positive infinity for all w values less than 0).
      // Since Z is not needed in this NDC value currently though we can just set the X/Y to be 0. This doesn't make a ton of sense as really the
      // camera's central point should only be a single pixel on the screen rather than stretched across the whole thing, but floating point quantization
      // will essentially cause much of the screen intersecting with geometry to map to a single point anyways, so this is probably fine.
      prevNDC = vec2(0.0f, 0.0f);
    } else {
      // Note: Typical perspective divide.
      prevNDC = prevClip.xy / prevClip.w;
    }

    const vec2 ndc = cameraPixelCoordinateToNDC(cb.camera, pixelCoordinate);
    const vec2 motionVectorNDC = prevNDC - ndc;
    const vec2 motionVectorUVOffset = motionVectorNDC * vec2(0.5, -0.5);
    const vec2 motionVectorPixel = motionVectorUVOffset * cb.camera.resolution;

    // Write out screen-space motion.
    PrimaryScreenSpaceMotionVector[pixelCoordinate] = motionVectorPixel;

    const float luminance = calcBt709Luminance(radiance);
    const bool isRadianceOutput = luminance > 0.0f;

    // Write out screen-space static bit in surface flags
    MotionBlurSurfaceFlags motionBlurSurfaceFlags;
    motionBlurSurfaceFlags.isViewModel = isViewModel;
    motionBlurSurfaceFlags.isStatic = surface.isStatic;
    motionBlurSurfaceFlags.isEmissive = isRadianceOutput;
    motionBlurSurfaceFlagsWriteToGBuffer(motionBlurSurfaceFlags, ivec2(pixelCoordinate), PrimarySurfaceFlags);
  }
  else
  {
    imageStore(SecondaryVirtualMotionVector, ivec2(pixelCoordinate), vec4(virtualMotion, 0));
  }

  // Serialize out the Ray Interaction (Primary surface coherent, secondary surface stochastic)

  if (primarySurface)
  {
    minimalRayInteractionWriteToGBuffer(
      rayInteraction, directionAltered, ivec2(pixelCoordinate),
      PrimaryViewDirection, PrimaryConeRadius);
  }
  else if (selectedIntegrationSurface)
  {
    minimalRayInteractionWriteToGBuffer(
      rayInteraction, directionAltered, ivec2(pixelCoordinate),
      SecondaryViewDirection, SecondaryConeRadius);
  }

  // Serialize out the Surface Interaction (Primary surface coherent, secondary surface stochastic)

  if (primarySurface)
  {
    minimalSurfaceInteractionWriteToGBuffer(
      surfaceInteraction, ivec2(pixelCoordinate),
      aliasedData0.PrimaryWorldPositionWorldTriangleNormal,
      PrimaryPositionError);
    PrimaryWorldInterpolatedNormal[pixelCoordinate] = float2x32ToSnorm2x16(sphereDirectionToSignedOctahedral(surfaceInteraction.interpolatedNormal));
  }
  else if (selectedIntegrationSurface)
  {
    minimalSurfaceInteractionWriteToGBuffer(
      surfaceInteraction, ivec2(pixelCoordinate),
      SecondaryWorldPositionWorldTriangleNormal,
      SecondaryPositionError);
  }

  // Serialize out the Medium Material Index
  // Note: This is done to share the medium material index with subsequent integration passes as they need to
  // know which medium the path is currently traveling through.

  if (selectedIntegrationSurface)
  {
    imageStore(SharedMediumMaterialIndex, ivec2(pixelCoordinate), mediumMaterialIndex);
  }

  // Serialize out the Surface Material Interaction

  if (primarySurface)
  {
    // Note: Always write to stochastic buffers in the primary surface case.
    polymorphicSurfaceMaterialInteractionWriteToGBuffer(
      polymorphicSurfaceMaterialInteraction, ivec2(pixelCoordinate), true,
      surface, surfaceInteraction, rayInteraction.surfaceIndex,
      directionAltered, accumulatedRotation,
      PrimaryWorldShadingNormal, PrimaryPerceptualRoughness,
      PrimaryVirtualWorldShadingNormalPerceptualRoughness, PrimaryVirtualWorldShadingNormalPerceptualRoughnessDenoising,
      PrimaryAlbedo, PrimaryBaseReflectivity,
      SharedMaterialData0, SharedMaterialData1, SharedTextureCoord, SharedSurfaceIndex, SharedSubsurfaceData, SharedSubsurfaceDiffusionProfileData);
  }
  else
  {
    // Note: Only write to stochastic buffers if this is the selected surface in the secondary surface case.
    polymorphicSurfaceMaterialInteractionWriteToGBuffer(
      polymorphicSurfaceMaterialInteraction, ivec2(pixelCoordinate), selectedIntegrationSurface,
      surface, surfaceInteraction, rayInteraction.surfaceIndex,
      directionAltered, accumulatedRotation,
      SecondaryWorldShadingNormal, SecondaryPerceptualRoughness,
      SecondaryVirtualWorldShadingNormalPerceptualRoughness, SecondaryVirtualWorldShadingNormalPerceptualRoughnessDenoising,
      SecondaryAlbedo, SecondaryBaseReflectivity,
      SharedMaterialData0, SharedMaterialData1, SharedTextureCoord, SharedSurfaceIndex, SharedSubsurfaceData, SharedSubsurfaceDiffusionProfileData);
  }

  if (cb.enableStochasticAlphaBlend && primarySurface)
  {
    //Write opaque objects' geometry hash
    AlphaBlendSurface alphaBlendSurface = AlphaBlendSurface.createFromPacked(AlphaBlendGBuffer[pixelCoordinate]);
    alphaBlendSurface.finalize(surface.hashPacked);
    AlphaBlendGBuffer[pixelCoordinate] = alphaBlendSurface.pack();
  }

  geometryResolverOutputSurfaceDebugView(pixelCoordinate, virtualMotion);
}

void geometryResolverDLSSRROutputSurface(
    uvec2 pixelCoordinate,
    Ray ray,
    Surface surface, SurfaceInteraction surfaceInteraction,
    PolymorphicSurfaceMaterialInteraction polymorphicSurfaceMaterialInteraction,
    f16vec3 attenuation, f16vec4 accumulatedRotation, float virtualHitDistance)
{
  // Recreate the original primary camera ray
  // Note: This is done to not have to pass extra information into the payload.

  const Ray originalCameraRay = rayCreatePrimaryFromPixel(cb.camera, pixelCoordinate, false);

  // Note: Using virtual hit distance accumulated through PSR and Ray Portals used to calculate a virtual hit position
  const vec3 virtualHitPosition = rayEvaluate(originalCameraRay, virtualHitDistance);
  const float linearViewZ = cameraWorldPosToLinearViewZ(cb.camera, virtualHitPosition);

  const float4 projectionSpacePosition = mul(cb.camera.worldToProjectionJittered, float4(virtualHitPosition, 1.f));
  const float depth = projectionSpacePosition.z / projectionSpacePosition.w;
  imageStore(PrimaryDepthDLSSRR, pixelCoordinate, depth);

  // Output Motion Vector (Primary/secondary surface coherent)
  // Note: Technically part of the Surface Interaction, just not something we'd want to deserialize in other passes which would mean we'd need yet
  // another structure for this. To avoid that, just output it like this for now.

  // Note: Virtual motion used to account for the ray traversing ray portals and through PSR interactions.
  const vec3 virtualMotion = quaternionTransformVector(accumulatedRotation, surfaceInteraction.motion, true);
  
  vec4 prevWorldPosition = vec4(virtualHitPosition + virtualMotion, 1);
  vec4 prevNDC = mul(cb.camera.prevWorldToProjection, prevWorldPosition);
  prevNDC /= prevNDC.w;

  vec2 ndc = cameraPixelCoordinateToNDC(cb.camera, pixelCoordinate);
  vec2 motionVectorNDC = prevNDC.xy - ndc.xy;
  vec2 motionVectorUVOffset = motionVectorNDC * vec2(0.5, -0.5);
  vec2 motionVectorPixel = motionVectorUVOffset * cb.camera.resolution;

  // Write out screen-space motion.
  PrimaryScreenSpaceMotionVectorDLSSRR[pixelCoordinate] = motionVectorPixel;

  // Serialize out the Surface Material Interaction
  // Note: Always write to stochastic buffers in the primary surface case.
  polymorphicSurfaceMaterialInteractionWriteToGBufferDLSSRR(
      polymorphicSurfaceMaterialInteraction, ivec2(pixelCoordinate), surface,
      PrimaryWorldShadingNormalDLSSRR);
}

void accumulateParticleBuffer(ivec2 pixelCoordinate, vec3 radiance)
{
  // Accumulate radiance to particle buffer, when DLSS-RR is on.
  // Particles, glasses and opaque surfaces' emissive component will be recorded in this buffer.
  vec4 prevParticleEmissive = ParticleBuffer[pixelCoordinate];
  prevParticleEmissive.xyz += radiance;
  ParticleBuffer[pixelCoordinate] = prevParticleEmissive;
}

void geometryResolverOutputSurfaceDebugView(
  uvec2 pixelCoordinate,
  vec3 virtualMotion)
 {
  switch(cb.debugView)
  {
  default:
  case DEBUG_VIEW_DISABLED:
    break;
  case DEBUG_VIEW_VIRTUAL_MOTION_VECTOR:
    storeInDebugView(pixelCoordinate, virtualMotion);
    break;
   case DEBUG_VIEW_NAN:
   {
     bool isValid = true;
      
     // DEBUG_VIEW_VIRTUAL_MOTION_VECTOR
     isValid &= isValidValue(virtualMotion);

     accumulateInDebugViewAnd(pixelCoordinate, isValid);
     break;
   }
  }
}

void geometryResolverVertexOutputDebugView(
  uvec2 pixelCoordinate,
  Ray ray, RayInteraction rayInteraction,
  Surface surface, SurfaceInteraction surfaceInteraction,
  PolymorphicSurfaceMaterialInteraction polymorphicSurfaceMaterialInteraction,
  float16_t selectedIntegrationSurfacePdf,
  bool directionAltered, PortalSpace2BitsType portalSpace, float virtualHitDistance,
  const bool isRaytracedRenderTarget)
{
  const uint8_t materialType = polymorphicSurfaceMaterialInteractionGetTypeHelper(polymorphicSurfaceMaterialInteraction);

  switch(cb.debugView)
  {
  default:
  case DEBUG_VIEW_DISABLED:
    break;
  case DEBUG_VIEW_PRIMITIVE_INDEX:

    storeInDebugView(pixelCoordinate, vec3(
      float(rayInteraction.primitiveIndex & 0xFF) / 256.f,
      float((rayInteraction.primitiveIndex >> 8) & 0xFF) / 256.f,
      float((rayInteraction.primitiveIndex >> 16) & 0xFF) / 256.f));

    break;
  case DEBUG_VIEW_CUSTOM_INDEX:

    if (rayInteraction.customIndex & CUSTOM_INDEX_IS_VIEW_MODEL)
      storeInDebugView(pixelCoordinate, vec3(0.f, 0.f, 1.f));
    else 
      storeInDebugView(pixelCoordinate, vec3(
        float(rayInteraction.customIndex & 0x3FF) / 1024.f,
        float((rayInteraction.customIndex >> 10) & 0x3FF) / 1024.f,
        float((rayInteraction.customIndex >> 20) & 0xFFF) / 4096.f));

    break;
  case DEBUG_VIEW_BARYCENTRICS:
    const vec3 bary = uintToBarycentrics(rayInteraction.barycentricCoordinates);

    storeInDebugView(pixelCoordinate, bary);

    break;
  case DEBUG_VIEW_IS_FRONT_HIT:
    // Green = front, Red = back
    storeInDebugView(pixelCoordinate, rayInteraction.frontHit ? vec3(0.0, 1.0, 0.0) : vec3(1.0, 0.0, 0.0));

    break;
  case DEBUG_VIEW_IS_STATIC:
    // Green = static, Red = not static
    storeInDebugView(pixelCoordinate, surface.isStatic ? vec3(0.0, 1.0, 0.0) : vec3(1.0, 0.0, 0.0));

    break;
  case DEBUG_VIEW_IS_OPAQUE:
    // Green = opaque, Red = not opaque
    storeInDebugView(pixelCoordinate, surface.isFullyOpaque ? vec3(0.0, 1.0, 0.0) : vec3(1.0, 0.0, 0.0));

    break;
  case DEBUG_VIEW_IS_DIRECTION_ALTERED:
    // Green = direction altered, Red = direction not altered
    storeInDebugView(pixelCoordinate, directionAltered ? vec3(0.0, 1.0, 0.0) : vec3(1.0, 0.0, 0.0));

    break;
  case DEBUG_VIEW_IS_EMISSIVE_BLEND:
    // Green = Emissive blend mode in use, Red = non emissive blend mode in use
    storeInDebugView(pixelCoordinate, surface.isEmissiveBlend ? vec3(0.0, 1.0, 0.0) : vec3(1.0, 0.0, 0.0));

    break;
  case DEBUG_VIEW_IS_EMISSIVE:
    // Green = Emissive mode in use, Red = non emissive mode in use
    storeInDebugView(pixelCoordinate, surface.isEmissive ? vec3(0.0, 1.0, 0.0) : vec3(1.0, 0.0, 0.0));

    break;
  case DEBUG_VIEW_IS_PARTICLE:
    // Green = Particle mode in use, Red = non particle mode in use
    storeInDebugView(pixelCoordinate, surface.isParticle ? vec3(0.0, 1.0, 0.0) : vec3(1.0, 0.0, 0.0));

    break;
  case DEBUG_VIEW_VIEW_DIRECTION:
    storeInDebugView(pixelCoordinate, rayInteraction.viewDirection);

    break;
  case DEBUG_VIEW_CONE_RADIUS:
    storeInDebugView(pixelCoordinate, rayInteraction.coneRadius);

    break;
  case DEBUG_VIEW_POSITION:
    storeInDebugView(pixelCoordinate, fract(surfaceInteraction.position));

    break;
  case DEBUG_VIEW_TEXCOORDS:
    // Note: Only fractional part of texture coordinates matter when wrapped. This won't represent
    // textures which use a clamp to edge blend mode super well, but that is fine.
    storeInDebugView(pixelCoordinate, fract(surfaceInteraction.textureCoordinates));

    break;
  case DEBUG_VIEW_TEXCOORDS_GRADIENT_X:
    storeInDebugView(pixelCoordinate, surfaceInteraction.textureGradientX);
    break;
  case DEBUG_VIEW_TEXCOORDS_GRADIENT_Y:
    storeInDebugView(pixelCoordinate, surfaceInteraction.textureGradientY);
    break;
  case DEBUG_VIEW_TEXCOORD_GENERATION_MODE:
    storeInDebugView(pixelCoordinate, uint(surface.texcoordGenerationMode));
    break;
  case DEBUG_VIEW_TRIANGLE_NORMAL:
    storeInDebugView(pixelCoordinate, surfaceInteraction.triangleNormal);
    break;
  case DEBUG_VIEW_TRIANGLE_TANGENT:
    storeInDebugView(pixelCoordinate, surfaceInteraction.triangleTangent);
    break;
  case DEBUG_VIEW_TRIANGLE_BITANGENT:
    storeInDebugView(pixelCoordinate, surfaceInteraction.triangleBitangent);
    break;
  case DEBUG_VIEW_INTERPOLATED_NORMAL:
    storeInDebugView(pixelCoordinate, surfaceInteraction.interpolatedNormal);
    break;
  case DEBUG_VIEW_INTERPOLATED_TANGENT:
    storeInDebugView(pixelCoordinate, surfaceInteraction.interpolatedTangent);
    break;
  case DEBUG_VIEW_INTERPOLATED_BITANGENT:
    storeInDebugView(pixelCoordinate, surfaceInteraction.interpolatedBitangent);
    break;
  case DEBUG_VIEW_SHADING_NORMAL:
    f16vec3 shadingNormal = f16vec3(0.0f);

    if (materialType == surfaceMaterialTypeOpaque)
    {
      shadingNormal = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).shadingNormal;
    }
    else if (materialType == surfaceMaterialTypeTranslucent)
    {
      shadingNormal = translucentSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).shadingNormal;
    }

    storeInDebugView(pixelCoordinate, shadingNormal);

    break;
  case DEBUG_VIEW_VERTEX_COLOR:
    storeInDebugView(pixelCoordinate, surfaceInteraction.vertexColor);

    break;
  case DEBUG_VIEW_VERTEX_ALPHA:
    storeInDebugView(pixelCoordinate, surfaceInteraction.vertexColor.w);

    break;
  case DEBUG_VIEW_PORTAL_SPACE:
  {
    f16vec3 typeColor = f16vec3(0.0f);
    switch (uint(portalSpace)) {
    case PORTAL_SPACE_NONE: typeColor = f16vec3(0.25f, 0.25f, 0.25f);         // Gray
      break;
    case PORTAL_SPACE_PORTAL_0: typeColor = f16vec3(1.0f, 0.0f, 0.0f);        // Red
      break;
    case PORTAL_SPACE_PORTAL_1: typeColor = f16vec3(0.0f, 1.0f, 0.0f);        // Green
      break;
    case PORTAL_SPACE_PORTAL_COMBINED: typeColor = f16vec3(0.0f, 0.0f, 1.0f); // Blue
      break;
    }
    
    storeInDebugView(pixelCoordinate, typeColor);

    break;
  }
  case DEBUG_VIEW_MATERIAL_TYPE: 
  {
    // Green = Opaque, Blue = Translucent, Red = Ray Portal (Shouldn't be seen)
    f16vec3 typeColor = f16vec3(0.0f);

    if (materialType == surfaceMaterialTypeOpaque)
    {
      typeColor = f16vec3(0.0f, 1.0f, 0.0f);
    }
    else if (materialType == surfaceMaterialTypeTranslucent)
    {
      typeColor = f16vec3(0.0f, 0.0f, 1.0f);
    }
    else if (materialType == surfaceMaterialTypeRayPortal)
    {
      typeColor = f16vec3(1.0f, 0.0f, 0.0f);
    }

    storeInDebugView(pixelCoordinate, typeColor);

    break;
  }
  case DEBUG_VIEW_ALBEDO:
    f16vec3 albedo = f16vec3(0.0f);

    if (materialType == surfaceMaterialTypeOpaque)
    {
      albedo = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).albedo;
    }

    storeInDebugView(pixelCoordinate, albedo);

    break;
  case DEBUG_VIEW_BASE_REFLECTIVITY:
    f16vec3 baseReflectivity = f16vec3(0.0f);

    if (materialType == surfaceMaterialTypeOpaque)
    {
      baseReflectivity = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).baseReflectivity;
    }
    else if (materialType == surfaceMaterialTypeTranslucent)
    {
      baseReflectivity = f16vec3(translucentSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).baseReflectivity);
    }
    
    storeInDebugView(pixelCoordinate, baseReflectivity);

    break;
  case DEBUG_VIEW_ROUGHNESS:
    float16_t isotropicRoughness = float16_t(0.0f);

    if (materialType == surfaceMaterialTypeOpaque)
    {
      isotropicRoughness = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).isotropicRoughness;
    }
    
    storeInDebugView(pixelCoordinate, isotropicRoughness);

    break;
  case DEBUG_VIEW_ANISOTROPY:
    float16_t anisotropy = float16_t(0.0f);

    if (materialType == surfaceMaterialTypeOpaque)
    {
      OpaqueSurfaceMaterialInteraction opaqueSurfaceMaterialInteraction = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction);
      anisotropy = anisotropicRoughnessToAnisotropy(opaqueSurfaceMaterialInteraction.isotropicRoughness, opaqueSurfaceMaterialInteraction.anisotropicRoughness);
    }
    
    storeInDebugView(pixelCoordinate, anisotropy);

    break;
  case DEBUG_VIEW_ANISOTROPIC_ROUGHNESS:
    f16vec2 anisotropicRoughness = f16vec2(0.0f, 0.0f);

    if (materialType == surfaceMaterialTypeOpaque)
    {
      anisotropicRoughness = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).anisotropicRoughness;
    }
    
    storeInDebugView(pixelCoordinate, anisotropicRoughness);

    break;
  case DEBUG_VIEW_OPACITY:
    float16_t opacity = float16_t(0.0f);

    if (materialType == surfaceMaterialTypeOpaque)
    {
      opacity = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).opacity;
    }
    
    storeInDebugView(pixelCoordinate, opacity);

    break;
  case DEBUG_VIEW_EMISSIVE_RADIANCE:
    f16vec3 emissiveRadiance = f16vec3(0.0f);

    if (materialType == surfaceMaterialTypeOpaque)
    {
      emissiveRadiance = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).emissiveRadiance;
    }
    else if (materialType == surfaceMaterialTypeTranslucent)
    {
      emissiveRadiance = translucentSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).emissiveRadiance;
    }

    storeInDebugView(pixelCoordinate, emissiveRadiance);

    break;

  case DEBUG_VIEW_EMISSIVE_TRIANGLE_INTENSITY:
  {
    vec3 lightIntensity = f16vec3(0.0f);

    if (materialType == surfaceMaterialTypeOpaque)
    {
      vec3 triangleCenter;
      f16vec3 triangleNormal;
      NEECacheUtils.calculateTriangleLightIntensity(rayInteraction.surfaceIndex, rayInteraction.primitiveIndex, triangleCenter, triangleNormal, lightIntensity);
    }

    storeInDebugView(pixelCoordinate, lightIntensity);
    break;
  }
  case DEBUG_VIEW_EMISSIVE_PARTICLE:
  {
    storeInDebugView(pixelCoordinate, SharedBiasCurrentColorMask[pixelCoordinate]);
    break;
  }
  case DEBUG_VIEW_NEE_CACHE_SAMPLE_RADIANCE:
  {
    vec3 sampleRadiance = f16vec3(0.0f);

    if (materialType == surfaceMaterialTypeOpaque)
    {
      float lightObjectPdf = 1.0;
      vec3 baryCentrics = uintToBarycentrics(rayInteraction.barycentricCoordinates);
      float triangleAreaUnused;
      LightSample lightSample = NEECacheUtils.calculateLightSampleFromTriangle(
        rayInteraction.surfaceIndex, rayInteraction.primitiveIndex, baryCentrics.yz, lightObjectPdf, cameraGetWorldPosition(cb.camera), rayInteraction.coneRadius, ray.spreadAngle, triangleAreaUnused);
      sampleRadiance = lightSample.radiance;
    }

    storeInDebugView(pixelCoordinate, sampleRadiance);
    break;
  }
  case DEBUG_VIEW_NEE_CACHE_TRIANGLE_CANDIDATE:
  {
    storeInDebugView(pixelCoordinate, asfloat(uint4(rayInteraction.surfaceIndex, rayInteraction.primitiveIndex, 0, 0)));
    break;
  }

  case DEBUG_VIEW_SURFACE_AREA:
    storeInDebugView(pixelCoordinate, surfaceInteraction.triangleArea);
    break;
  case DEBUG_VIEW_THIN_FILM_THICKNESS:
    float16_t thinFilmThickness = float16_t(0.0f);

    if (materialType == surfaceMaterialTypeOpaque)
    {
      thinFilmThickness = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).thinFilmThickness;
    }
    
    storeInDebugView(pixelCoordinate, thinFilmThickness);

    break;
  case DEBUG_VIEW_VIRTUAL_HIT_DISTANCE:
    storeInDebugView(pixelCoordinate, virtualHitDistance);

    break;
  case DEBUG_VIEW_PRIMARY_DEPTH:
    storeInDebugView(pixelCoordinate, PrimaryDepth[pixelCoordinate]);

    break;  
  case DEBUG_VIEW_PIXEL_CHECKERBOARD:
    // Note: Creates a pixel-perfect checkerboard pattern. Useful for identifying incorrect scaling
    // or pixel sampling as this will create visibile Moire Patterns in the result.
    const float continueSample = float((pixelCoordinate.x & 1) ^ (pixelCoordinate.y & 1));

    storeInDebugView(pixelCoordinate, continueSample);

    break;
  case DEBUG_VIEW_VOLUME_RADIANCE_DEPTH_LAYERS: {
    const Camera camera = cb.camera;
    const VolumeArgs volumeArgs = cb.volumeArgs;

    vec2 screenUV = cameraPixelCoordinateToScreenUV(camera, pixelCoordinate);
    screenUV.x *= cb.volumeArgs.numFroxelVolumes;
    const float froxelVolume = floor(screenUV.x);
    screenUV.x = frac(screenUV.x);
    const uint widthTiles = ceil(sqrt(float(volumeArgs.froxelDepthSlices)));
    const uint heightTiles = ceil(float(volumeArgs.froxelDepthSlices) / widthTiles);
    const uint previewLevel = uint(screenUV.x * widthTiles) + uint(screenUV.y * heightTiles) * widthTiles;

    vec3 filteredRadiance;

    if (previewLevel >= volumeArgs.froxelDepthSlices)
    {
      // Note: Output black to indicate no data for this section of the screen.
      filteredRadiance = vec3(0.0f, 0.0f, 0.0f);
    }
    else
    {
      vec2 previewUV = mod(screenUV, vec2(1.0f / widthTiles, 1.0f / heightTiles)) * vec2(widthTiles, heightTiles);
      previewUV.x = clamp(previewUV.x, volumeArgs.minFilteredRadianceU, volumeArgs.maxFilteredRadianceU);
      previewUV.x = (previewUV.x + froxelVolume) * volumeArgs.inverseNumFroxelVolumes;
      const float previewW = float(previewLevel) / float(volumeArgs.froxelDepthSlices);
      const float3 uvw = vec3(previewUV, previewW);
      SphericalHarmonic sh = HarmonicsHelpers::loadFiltered3D<SphericalHarmonic>(VolumeFilteredRadianceY, VolumeFilteredRadianceCoCg, uvw);
      if(cb.debugKnob.x <= 1.f)
      {
        filteredRadiance = sh.getIrradiance();
      } 
      else if(cb.debugKnob.x <= 2.f)
      {
        filteredRadiance = VolumeFilteredRadianceY.SampleLevel(uvw, 0).w;
      }
      else if(cb.debugKnob.x <= 3.f)
      {
        filteredRadiance = VolumeFilteredRadianceY.SampleLevel(uvw, 0).xyz;
      }
      else if(cb.debugKnob.x <= 4.f)
      {
        filteredRadiance.xy = VolumeFilteredRadianceCoCg.SampleLevel(uvw, 0);
      }
    }

    storeInDebugView(pixelCoordinate, filteredRadiance);

    break;
  }
  case DEBUG_VIEW_SURFACE_VOLUME_RADIANCE: {
    const uint froxelVolumeHint = portalSpaceToVolumeHint(portalSpace);

    vec3 filteredRadiance = evalVolumetricNEE(VolumeFilteredRadianceY, VolumeFilteredRadianceCoCg, cb.volumeArgs, 
      surfaceInteraction, polymorphicSurfaceMaterialInteraction.shadingNormal, froxelVolumeHint);

    // Show froxels
    if (cb.debugKnob.x > 0)
    {
      const VolumeDefinitionCamera camera = cb.volumeArgs.cameras[0];
      const vec3 translatedWorldPosition = worldToTranslatedWorld(camera, surfaceInteraction.position);
      const vec3 virtualFroxelUVW = translatedWorldPositionToFroxelUVW(
        camera.translatedWorldToView, camera.translatedWorldToProjectionJittered, packedFlagGet(camera.flags, rightHandedFlag),
        cb.volumeArgs.froxelDepthSlices, cb.volumeArgs.froxelDepthSliceDistributionExponent, cb.volumeArgs.froxelMaxDistance, camera.nearPlane,
        translatedWorldPosition);

      uvec3 froxelDimension = uvec3(cb.volumeArgs.froxelGridDimensions, cb.volumeArgs.froxelDepthSlices);
      if (cb.debugKnob.x > 1.0)
      {
        froxelDimension = uvec3(cb.volumeArgs.restirFroxelGridDimensions, cb.volumeArgs.restirFroxelDepthSlices);
      }
      uvec3 froxelCoordinate = froxelUVWToFroxelCoordinate(virtualFroxelUVW, froxelDimension);

      const int colorResolution = 8;
      filteredRadiance = (float3(froxelCoordinate.xy, 0) % colorResolution) / float(colorResolution);
    }

    storeInDebugView(pixelCoordinate, filteredRadiance);
    break;
  }
  case DEBUG_VIEW_PSR_SELECTED_INTEGRATION_SURFACE_PDF: 
    storeInDebugView(pixelCoordinate, selectedIntegrationSurfacePdf);
    break;
  case DEBUG_VIEW_GEOMETRY_HASH:
    storeInDebugView(pixelCoordinate, r5g6b5ToColor(surface.hashPacked)); // visualize directly as colour (note the hash is compressed into a 16bit using XOR)
    break;
  case DEBUG_VIEW_NAN:
    {
      bool isValid = true;
      
      // Skipped
      // DEBUG_VIEW_PRIMITIVE_INDEX
      // DEBUG_VIEW_CUSTOM_INDEX

      // DEBUG_VIEW_BARYCENTRICS
      isValid &= isValidValue(uintToBarycentrics(rayInteraction.barycentricCoordinates));
      
      // Skipped
      // DEBUG_VIEW_IS_FRONT_HIT
      // DEBUG_VIEW_IS_STATIC
      // DEBUG_VIEW_IS_OPAQUE
      // DEBUG_VIEW_IS_DIRECTION_ALTERED
      // DEBUG_VIEW_IS_EMISSIVE_BLEND
      // DEBUG_VIEW_IS_EMISSIVE
      // DEBUG_VIEW_IS_PARTICLE
      // DEBUG_VIEW_IS_RAY_PORTAL
      
      // DEBUG_VIEW_VIEW_DIRECTION
      isValid &= isValidValue(rayInteraction.viewDirection);

      // DEBUG_VIEW_CONE_RADIUS
      isValid &= isValidValue(rayInteraction.coneRadius);

      // DEBUG_VIEW_POSITION
      isValid &= isValidValue(surfaceInteraction.position);

      // DEBUG_VIEW_TEXCOORDS
      isValid &= isValidValue(surfaceInteraction.textureCoordinates);
    
      // DEBUG_VIEW_TEXCOORDS_GRADIENT_X
      isValid &= isValidValue(surfaceInteraction.textureGradientX);

      // DEBUG_VIEW_TEXCOORDS_GRADIENT_Y
      isValid &= isValidValue(surfaceInteraction.textureGradientY);

      // DEBUG_VIEW_TEXCOORD_GENERATION_MODE
      isValid &= isValidValue(surface.texcoordGenerationMode);

      // DEBUG_VIEW_TRIANGLE_NORMAL
      isValid &= isValidValue(surfaceInteraction.triangleNormal);

      // DEBUG_VIEW_TRIANGLE_TANGENT
      isValid &= isValidValue(surfaceInteraction.triangleTangent);

      // DEBUG_VIEW_TRIANGLE_BITANGENT
      isValid &= isValidValue(surfaceInteraction.triangleBitangent);

      // DEBUG_VIEW_INTERPOLATED_NORMAL
      isValid &= isValidValue(surfaceInteraction.interpolatedNormal);

      // DEBUG_VIEW_INTERPOLATED_TANGENT
      isValid &= isValidValue(surfaceInteraction.interpolatedTangent);

      // DEBUG_VIEW_INTERPOLATED_BITANGENT
      isValid &= isValidValue(surfaceInteraction.interpolatedBitangent);

      // DEBUG_VIEW_SHADING_NORMAL
      f16vec3 shadingNormal = f16vec3(0.0f);
      if (materialType == surfaceMaterialTypeOpaque)
      {
        shadingNormal = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).shadingNormal;
      }
      else if (materialType == surfaceMaterialTypeTranslucent)
      {
        shadingNormal = translucentSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).shadingNormal;
      }
      isValid &= isValidValue(shadingNormal);

      // DEBUG_VIEW_VERTEX_COLOR
      isValid &= isValidValue(surfaceInteraction.vertexColor);

      // Skipped
      // DEBUG_VIEW_VERTEX_ALPHA (already checked in DEBUG_VIEW_VERTEX_COLOR)
      // DEBUG_VUEW_PORTAL_SPACE
      // DEBUG_VIEW_MATERIAL_TYPE

      // DEBUG_VIEW_ALBEDO
      f16vec3 albedo = f16vec3(0.0f);
      if (materialType == surfaceMaterialTypeOpaque)
      {
        albedo = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).albedo;
      }
      isValid &= isValidValue(albedo);

      // DEBUG_VIEW_BASE_REFLECTIVITY
      f16vec3 baseReflectivity = f16vec3(0.0f);
      if (materialType == surfaceMaterialTypeOpaque)
      {
        baseReflectivity = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).baseReflectivity;
      }
      else if (materialType == surfaceMaterialTypeTranslucent)
      {
        baseReflectivity = f16vec3(translucentSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).baseReflectivity);
      }
      isValid &= isValidValue(baseReflectivity);

      // DEBUG_VIEW_ROUGHNESS
      float16_t isotropicRoughness = float16_t(0.0f);
      if (materialType == surfaceMaterialTypeOpaque)
      {
        isotropicRoughness = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).isotropicRoughness;
      }
      isValid &= isValidValue(isotropicRoughness);

      // DEBUG_VIEW_ANISOTROPY
      float16_t anisotropy = float16_t(0.0f);
      if (materialType == surfaceMaterialTypeOpaque)
      {
        OpaqueSurfaceMaterialInteraction opaqueSurfaceMaterialInteraction = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction);
        anisotropy = anisotropicRoughnessToAnisotropy(opaqueSurfaceMaterialInteraction.isotropicRoughness, opaqueSurfaceMaterialInteraction.anisotropicRoughness);
      }
      isValid &= isValidValue(anisotropy);

      // DEBUG_VIEW_ANISOTROPIC_ROUGHNESS
      f16vec2 anisotropicRoughness = f16vec2(0.0f, 0.0f);
      if (materialType == surfaceMaterialTypeOpaque)
      {
        anisotropicRoughness = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).anisotropicRoughness;
      }
      isValid &= isValidValue(anisotropicRoughness);

      // DEBUG_VIEW_OPACITY
      float16_t opacity = float16_t(0.0f);
      if (materialType == surfaceMaterialTypeOpaque)
      {
        opacity = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).opacity;
      }
      isValid &= isValidValue(opacity);

      // DEBUG_VIEW_EMISSIVE_RADIANCE
      f16vec3 emissiveRadiance = f16vec3(0.0f);
      if (materialType == surfaceMaterialTypeOpaque)
      {
        emissiveRadiance = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).emissiveRadiance;
      }
      else if (materialType == surfaceMaterialTypeTranslucent)
      {
        emissiveRadiance = translucentSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).emissiveRadiance;
      }
      isValid &= isValidValue(emissiveRadiance);

      // DEBUG_VIEW_EMISSIVE_TRIANGLE_INTENSITY:
      vec3 lightIntensity = f16vec3(0.0f);
      if (materialType == surfaceMaterialTypeOpaque)
      {
        vec3 triangleCenter;
        f16vec3 triangleNormal;
        NEECacheUtils.calculateTriangleLightIntensity(rayInteraction.surfaceIndex, rayInteraction.primitiveIndex, triangleCenter, triangleNormal, lightIntensity);
      } 
      isValid &= isValidValue(lightIntensity);
      
      // DEBUG_VIEW_EMISSIVE_PARTICLE:
      isValid &= isValidValue(SharedBiasCurrentColorMask[pixelCoordinate]);

      // DEBUG_VIEW_NEE_CACHE_SAMPLE_RADIANCE:
      vec3 sampleRadiance = f16vec3(0.0f);
      if (materialType == surfaceMaterialTypeOpaque)
      {
        float lightObjectPdf = 1.0;
        vec3 baryCentrics = uintToBarycentrics(rayInteraction.barycentricCoordinates);
        float triangleAreaUnused;
        LightSample lightSample = NEECacheUtils.calculateLightSampleFromTriangle(
          rayInteraction.surfaceIndex, rayInteraction.primitiveIndex, baryCentrics.yz, lightObjectPdf, cameraGetWorldPosition(cb.camera), rayInteraction.coneRadius, ray.spreadAngle, triangleAreaUnused);
        sampleRadiance = lightSample.radiance;
      }
      isValid &= isValidValue(sampleRadiance);

      // DEBUG_VIEW_NEE_CACHE_TRIANGLE_CANDIDATE:
      isValid &= isValidValue(asfloat(uint4(rayInteraction.surfaceIndex, rayInteraction.primitiveIndex, 0, 0)));

      // DEBUG_VIEW_SURFACE_AREA
      isValid &= isValidValue(surfaceInteraction.triangleArea);
      
      // DEBUG_VIEW_THIN_FILM_THICKNESS
      float16_t thinFilmThickness = float16_t(0.0f);
      if (materialType == surfaceMaterialTypeOpaque)
      {
        emissiveRadiance = opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction).thinFilmThickness;
      }
      isValid &= isValidValue(thinFilmThickness);

      // DEBUG_VIEW_VIRTUAL_HIT_DISTANCE
      isValid &= isValidValue(virtualHitDistance);

      // DEBUG_VIEW_PRIMARY_DEPTH
      isValid &= isValidValue(PrimaryDepth[pixelCoordinate]);

      // DEBUG_VIEW_PIXEL_CHECKERBOARD
      const float continueSample = float((pixelCoordinate.x & 1) ^ (pixelCoordinate.y & 1));
      isValid &= isValidValue(continueSample);

      // Skipped
      // DEBUG_VIEW_VOLUME_RADIANCE_DEPTH_LAYERS

      // DEBUG_VIEW_SURFACE_VOLUME_RADIANCE
      const VolumeArgs volumeArgs = cb.volumeArgs;
      const VolumeDefinitionCamera camera = volumeArgs.cameras[froxelVolumeMain];
      const vec3 translatedWorldPosition = worldToTranslatedWorld(camera, surfaceInteraction.position);
      const vec3 froxelUVW = translatedWorldPositionToFroxelUVW(
        camera.translatedWorldToView, camera.translatedWorldToProjectionJittered, packedFlagGet(camera.flags, rightHandedFlag),
        volumeArgs.froxelDepthSlices, volumeArgs.froxelDepthSliceDistributionExponent, volumeArgs.froxelMaxDistance, camera.nearPlane,
        translatedWorldPosition);
          vec3 volumeRadiance;
          if (any(froxelUVW < vec3(0.0f)) || any(froxelUVW >= vec3(1.0f)))
          {
            // Note: Store red for UVW coordinates out of range to indicate the froxel grid is clipping the
            // world, generally this indicates the max distance should be increased to a reasonable point.
            volumeRadiance = vec3(1000.0f, 0.0f, 0.0f);
          }
          else
          {
            SphericalHarmonic sh = HarmonicsHelpers::loadFiltered3D<SphericalHarmonic>(VolumeFilteredRadianceY, VolumeFilteredRadianceCoCg, froxelUVW);
            volumeRadiance = sh.getIrradiance();
          }
      isValid &= isValidValue(volumeRadiance);
      
      // DEBUG_VIEW_PSR_SELECTED_INTEGRATION_SURFACE_PDF: 
      isValid &= isValidValue(selectedIntegrationSurfacePdf);

      // DEBUG_VIEW_GEOMETRY_HASH:
      isValid &= isValidValue(r5g6b5ToColor(surface.hashPacked)); // visualize directly as colour (note the hash is compressed into a 16bit using XOR)

      accumulateInDebugViewAnd(pixelCoordinate, isValid);
      break;
    }
  case DEBUG_VIEW_IS_INSIDE_FRUSTUM:
    storeInDebugView(pixelCoordinate, surface.isInsideFrustum ? vec3(0.0f, 1.0f, 0.0f) : vec3(1.0f, 0.0f, 0.0f));
    break;
  case DEBUG_VIEW_IS_OUTSIDE_AABB:
    vec3 relativePosition = abs(surfaceInteraction.position - cameraGetWorldPosition(cb.camera));
    storeInDebugView(pixelCoordinate, vec3(relativePosition > cb.debugKnob.xyz * 0.5f));
    break;
  case DEBUG_VIEW_NRC_IS_OUTSIDE_SCENE_AABB:
    bool isOutsideAabb = any(surfaceInteraction.position < cb.nrcArgs.sceneBoundsMin)
                      || any(surfaceInteraction.position > cb.nrcArgs.sceneBoundsMax);
      accumulateInDebugViewMax(pixelCoordinate, vec4(isOutsideAabb, 0, 0, 0));
    break;
  case DEBUG_VIEW_IS_THIN_OPAQUE:
    bool isThinOpaque = false;
    if (materialType == surfaceMaterialTypeOpaque)
    {
      isThinOpaque = isThinOpaqueSubsurfaceMaterial(opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction));
    }
    // Green = thin opaque, Red = not thin opaque
    storeInDebugView(pixelCoordinate, isThinOpaque ? vec3(0.0, 1.0, 0.0) : vec3(1.0, 0.0, 0.0));

    break;
  case DEBUG_VIEW_IS_SUBSURFACE_SCATTERING:
    bool isSss = false;
    if (materialType == surfaceMaterialTypeOpaque)
    {
      isSss = isSubsurfaceDiffusionProfileMaterial(opaqueSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction));
    }
    // Green = SSS material, Red = not SSS material
    storeInDebugView(pixelCoordinate, isSss ? vec3(0.0, 1.0, 0.0) : vec3(1.0, 0.0, 0.0));

    break;
  case DEBUG_VIEW_POM_ITERATIONS:
#ifdef OPAQUE_MATERIAL_USE_POM
    if (materialType == surfaceMaterialTypeOpaque && cb.pomMode != DisplacementMode::Off)
    {
      const MemoryPolymorphicSurfaceMaterial memoryPolymorphicSurfaceMaterial = surfaceMaterials[uint(rayInteraction.surfaceIndex)];
      OpaqueSurfaceMaterial opaqueSurfaceMaterial = opaqueSurfaceMaterialCreate(memoryPolymorphicSurfaceMaterial);
      float3 hitPos = float3(0.f);
      uint iterations = 0;
      if (opaqueSurfaceMaterial.hasValidDisplacement())
      {
        SurfaceInteraction tempSurfaceInteraction = surfaceInteractionCreate<SurfaceGenerateTangents>(surface, rayInteraction, ray);
        const f16mat3 worldToTangent = f16mat3(tempSurfaceInteraction.interpolatedTangent, tempSurfaceInteraction.interpolatedBitangent, tempSurfaceInteraction.interpolatedNormal);
        const f16vec3 viewDirTangentSpace = normalize(mul(worldToTangent, rayInteraction.viewDirection));
        hitPos = pomCalculateTexcoord(opaqueSurfaceMaterial, tempSurfaceInteraction, viewDirTangentSpace, iterations);
      }
      float iterFrac = ((float)iterations) / cb.pomMaxIterations;
      storeInDebugView(pixelCoordinate, float3(iterFrac, 1.f-iterFrac, 0.f));
    }
#endif
    break;
  case DEBUG_VIEW_POM_DIRECT_HIT_POS:
#ifdef OPAQUE_MATERIAL_USE_POM
    if (materialType == surfaceMaterialTypeOpaque && cb.pomMode != DisplacementMode::Off)
    {
      const MemoryPolymorphicSurfaceMaterial memoryPolymorphicSurfaceMaterial = surfaceMaterials[uint(rayInteraction.surfaceIndex)];
      OpaqueSurfaceMaterial opaqueSurfaceMaterial = opaqueSurfaceMaterialCreate(memoryPolymorphicSurfaceMaterial);
      float3 hitPos = float3(0.f);
      if (opaqueSurfaceMaterial.hasValidDisplacement())
      {
        SurfaceInteraction tempSurfaceInteraction = surfaceInteractionCreate<SurfaceGenerateTangents>(surface, rayInteraction, ray);
        const f16mat3 worldToTangent = f16mat3(tempSurfaceInteraction.interpolatedTangent, tempSurfaceInteraction.interpolatedBitangent, tempSurfaceInteraction.interpolatedNormal);
        const f16vec3 viewDirTangentSpace = normalize(mul(worldToTangent, rayInteraction.viewDirection));
        uint iterations = 0;
        hitPos = pomCalculateTexcoord(opaqueSurfaceMaterial, tempSurfaceInteraction, viewDirTangentSpace, iterations);
      }
      storeInDebugView(pixelCoordinate, hitPos);
    }
#endif
    break;
  case DEBUG_VIEW_HEIGHT_MAP:
#ifdef OPAQUE_MATERIAL_USE_POM
    float heightMapValue = -1.f;
    if (materialType == surfaceMaterialTypeOpaque && cb.pomMode != DisplacementMode::Off)
    {
      const MemoryPolymorphicSurfaceMaterial memoryPolymorphicSurfaceMaterial = surfaceMaterials[uint(rayInteraction.surfaceIndex)];
      OpaqueSurfaceMaterial opaqueSurfaceMaterial = opaqueSurfaceMaterialCreate(memoryPolymorphicSurfaceMaterial);
      if (opaqueSurfaceMaterial.hasValidDisplacement()) {
        heightMapValue = 1.f - pomSampleHeight(opaqueSurfaceMaterial.heightTextureIndex, opaqueSurfaceMaterial.samplerIndex, surfaceInteraction.textureCoordinates);
      }
    }
    
    storeInDebugView(pixelCoordinate, heightMapValue);
#endif
    break;
    
  case DEBUG_VIEW_RAYTRACED_RENDER_TARGET_GEOMETRY:
    if (cb.enableRaytracedRenderTarget && isRaytracedRenderTarget){
      storeInDebugView(pixelCoordinate, vec4(surfaceInteraction.textureCoordinates, 0.0, 1.0));
    }
    break;
  case DEBUG_VIEW_RAYTRACED_RENDER_TARGET_DIRECT:
    if (cb.enableRaytracedRenderTarget && isRaytracedRenderTarget){
      storeInDebugView(pixelCoordinate, vec4(1.f));
    }
    break;
  }
}

void geometryResolverOutputDebugView(uvec2 pixelCoordinate)
{
  switch(cb.debugView)
  {
  default:
    break;
  case DEBUG_VIEW_SHARED_BIAS_CURRENT_COLOR_MASK:
    storeInDebugView(pixelCoordinate, SharedBiasCurrentColorMask[pixelCoordinate]);
    break;
  case DEBUG_VIEW_NAN:
    {
      bool isValid = true;
      // DEBUG_VIEW_SHARED_BIAS_CURRENT_COLOR_MASK
      isValid &= isValidValue(SharedBiasCurrentColorMask[pixelCoordinate]);
      
      accumulateInDebugViewAnd(pixelCoordinate, isValid);
      break;
    }
  }
}

// This function calculates a diffuse layer contribution for the surface from a combined diffuse layer weight. Do note this weight is typically
// made up of a shared component and a component from the reflection and/or translucent side of things. This should be added in regardless of if reflection
// or transmission PSR is actually done, just since there's no need to make this contribution noisy for no reason. This contribution does not work quite
// properly however when reflection or transmission PSR are individually disabled right now, we'd need to conditionally have the diffuse layer weight
// only include the relevant lobe contributions rather than both always.
vec3 approximateDiffuseLayerRadiance(SurfaceInteraction surfaceInteraction,
                                     f16vec3 shadingNormal,
                                     PortalSpace2BitsType portalSpace,
                                     const f16vec3 diffuseLayerWeight)
{
  // Note: Important early out check to avoid expensive radiance cache lookups when they are not needed (as the diffuse layer
  // weight will be set to 0 in these cases).
  if (all(diffuseLayerWeight <= 0.0))
  {
    return vec3(0);
  }

  // Fetch data from the radiance cache for the current surface interaction

  const VolumeArgs volumeArgs = cb.volumeArgs;
  const uint froxelVolumeHint = portalSpaceToVolumeHint(portalSpace);
  const vec3 accumulatedOutgoingRadiance = evalVolumetricNEE(VolumeFilteredRadianceY, VolumeFilteredRadianceCoCg, volumeArgs,
    surfaceInteraction, shadingNormal, froxelVolumeHint);

  // Light the surface to add in a diffuse layer approximation

  // Note: Diffuse layer weight from the material model assumes the radiance cache contains a value pre-divided by 4pi as a basic isotropic
  // scattering model.
  // Todo: We should really be using spherical harmonics in the future for this to actually evaluate a directional contribution of the diffuse
  // layer as right now isotropic scattering looks a bit darker than it should (most the light should be backscattered for such a surface realistically
  // rather than equally scattered around).
  return diffuseLayerWeight * accumulatedOutgoingRadiance / 4;
}

// Performs NRC related updates after a GBuffer miss
void updateNrcOnGBufferMiss(
  uvec2 pixelCoordinate,
  NrcArgs nrcArgs)
{
#if ENABLE_NRC
  // Query path is always traced at the full ray tracing resolution
  const uvec2 queryPixelCoordinate = pixelCoordinate;
  
  // Update NRC Query Path
  {
    g_nrcMode = NrcMode::Query;

    Nrc::updateNrcOnPrimaryMiss(queryPixelCoordinate);
  }

  // Update NRC Training Path
  if (cb.allowNrcTraining)
  {
    uvec2 trainingPixelCoordinateInTrainingSpace;
    const bool hasTrainingPath = Nrc::calculateIfPixelHasActiveNrcTrainingPath(
      queryPixelCoordinate, cb.nrcArgs, trainingPixelCoordinateInTrainingSpace);

    // Training paths are more sparse than Query paths, so filter the updates to pixels that have a training path
    if (hasTrainingPath)
    {
      g_nrcMode = NrcMode::Update;

      Nrc::updateNrcOnPrimaryMiss(trainingPixelCoordinateInTrainingSpace);
    }
  }

  g_nrcMode = NrcMode::Disabled;
#endif
}

// Performs NRC related updates after a GBuffer hit
void updateNrcOnGBufferHitTrainingAndQueryPaths(
  uvec2 pixelCoordinate,
  vec3 rayDirection,
  float primaryPathHitDistance,
  SurfaceInteraction surfaceInteraction,
  PolymorphicSurfaceMaterialInteraction polymorphicSurfaceMaterialInteraction,
  // GBuffer surface radiance - emissive + diffuse layer (i.e. all except Integrate Direct and Indirect Lighting)
  vec3 gbufferSurfaceRadiance,
  inout RNG randomState)
{
#if ENABLE_NRC
  // Query path is always traced at the full ray tracing resolution
  const uvec2 queryPixelCoordinate = pixelCoordinate;
  
  // Update NRC Query Path
  {
    g_nrcMode = NrcMode::Query;

    const float rand01 = getNextSampleBlueNoise(randomState);

    Nrc::updateNrcOnPrimaryHit(queryPixelCoordinate, 
      surfaceInteraction, polymorphicSurfaceMaterialInteraction, -rayDirection, 
      primaryPathHitDistance, rand01, NrcQueryPathData0, NrcQueryPathData1);
  }

  // Update NRC Training Path
  if (cb.allowNrcTraining)
  {
    uvec2 trainingPixelCoordinateInTrainingSpace;
    const bool hasTrainingPath = Nrc::calculateIfPixelHasActiveNrcTrainingPath(
      queryPixelCoordinate, cb.nrcArgs, trainingPixelCoordinateInTrainingSpace);

    // Training paths are more sparse than Query paths, so filter the updates to pixels that have a training path
    if (hasTrainingPath)
    {
      g_nrcMode = NrcMode::Update;

      // Create a custom random state using training pixel coordinate for Update path 
      // since their query pixel coordinates counter parts are more spare
      // making the random samples worse distributed
      RNG randomStateNrcUpdate = createRNG(trainingPixelCoordinateInTrainingSpace, cb.frameIdx);
      const float rand01 = getNextSampleBlueNoise(randomStateNrcUpdate);

      Nrc::updateNrcOnPrimaryHit(trainingPixelCoordinateInTrainingSpace, 
        surfaceInteraction, polymorphicSurfaceMaterialInteraction, -rayDirection,
        // Note: NrcQueryPathData0 is unused for Update path
        primaryPathHitDistance, rand01, NrcQueryPathData0, NrcTrainingPathData1);

      // Store radiance accumulated on the surface during GBuffer pass, this will be added to accumulated radiance
      // between vertex 0 and vertex 1 for training paths in the indirect integrate pass
      NrcTrainingGBufferSurfaceRadianceRG[trainingPixelCoordinateInTrainingSpace] = gbufferSurfaceRadiance.rg;
      NrcTrainingGBufferSurfaceRadianceB[trainingPixelCoordinateInTrainingSpace] = gbufferSurfaceRadiance.b;
    }
  
    if (cb.debugView == DEBUG_VIEW_NRC_UPDATE_PIXEL)
    {
      storeInDebugView(queryPixelCoordinate, hasTrainingPath);
    }
  }

  g_nrcMode = NrcMode::Disabled;
#else
  unused(randomState);
#endif
}

void geometryResolverVertex(
  EmptyExtraArgs extraArgs, RayHitInfo rayHitInfo,
  inout GeometryResolverState geometryResolverState)
{
  // Setup resolve state

  // Note: Emissives are fine to approximate even on primary rays (since lighting rarely
  // matters on them), and non-emissive opacity particles should be lit with the lighting approximation.
  const uint8_t resolveMode =
    resolveModeAlteredDirectionNotify |
    resolveModeEmissiveOpacityTransmissionApprox |
    resolveModeDecalMaterialBlending |
    (cb.enableSeparateUnorderedApproximations ? resolveModeSeparateUnorderedApproximations : 0);
  f16vec3 radianceAttenuation = f16vec3(1.0f, 1.0f, 1.0f);
  vec3 emissiveRadiance = vec3(0);

  // Handle Unordered Resolving
  // Note: Done before resolveVertex so attenuation from unordered objects
  // between the origin and the hit point can be accumulated in advance.

  Ray unorderedResolveRay;
  unorderedResolveRay.origin = geometryResolverState.origin;
  unorderedResolveRay.coneRadius = geometryResolverState.coneRadius;
  unorderedResolveRay.spreadAngle = float16_t(cb.screenSpacePixelSpreadHalfAngle);
  unorderedResolveRay.direction = geometryResolverState.direction;
  unorderedResolveRay.tMax = rayHitInfo.hasHit ? rayHitInfo.hitDistance * kUnorderedResolveRayLengthening : floatMax;

  uint numUnorderedInteractions = 0;
  uint8_t unorderedRayMask = convertPrimaryRayMaskToUnordered(geometryResolverState.rayMask, OBJECT_MASK_ALL_UNORDERED);

  geometryResolverState.decalEncountered = false;
  
  // We only need to use billboards here when Portals are in play
#if SURFACE_MATERIAL_RESOLVE_TYPE_ACTIVE_MASK == SURFACE_MATERIAL_RESOLVE_TYPE_ALL
  #define GBUFFER_RAY_FLAGS (RAY_FLAG_CULL_OPAQUE)
  const bool useIntersectionBillboards = cb.enableBillboardOrientationCorrection && (geometryResolverState.directionAltered || cb.useIntersectionBillboardsOnPrimaryRays);
#else
  #define GBUFFER_RAY_FLAGS (RAY_FLAG_CULL_OPAQUE | RAY_FLAG_SKIP_PROCEDURAL_PRIMITIVES)
  const bool useIntersectionBillboards = false;
#endif
  
  uint4 decalMemory = 0;
  f16vec3 decalEmissiveRadiance = f16vec3(0);
  int surfaceIndex;
  resolveVertexUnordered<GBUFFER_RAY_FLAGS>(
    resolveMode, unorderedResolveRay,
    unorderedRayMask, geometryResolverState.portalSpace,
    useIntersectionBillboards,
    geometryResolverState.pixelCoordinate,
    geometryResolverState.accumulatedRotation, geometryResolverState.accumulatedHitDistance, 1.h,
    radianceAttenuation, emissiveRadiance,
    numUnorderedInteractions, 
    geometryResolverState.decalEncountered, decalMemory, decalEmissiveRadiance, surfaceIndex);

  bool ignoreTransparencyLayer = false;
  if (surfaceIndex >= 0)
  {
    const Surface surface = surfaces[uint(surfaceIndex)];
    ignoreTransparencyLayer = surface.ignoreTransparencyLayer;
  }

  vec3 particleEmissive = emissiveRadiance;

  if (cb.debugView == DEBUG_VIEW_PRIMARY_DECAL_ALBEDO)
  {
    MemoryDecalMaterialInteraction memory;
    memory.packed = decalMemory; 
    memory.emissiveRadiance = decalEmissiveRadiance;
    DecalMaterialInteraction decalMaterialInteraction = decalMaterialInteractionUnpack(memory);
    storeInDebugView(geometryResolverState.pixelCoordinate, decalMaterialInteraction.albedo);
  }

  // Note: Add to the number of unordered interactions in the debug view based on the number of unordered interactions in resolving.
  if (cb.debugView == DEBUG_VIEW_PRIMARY_UNORDERED_INTERACTIONS || cb.debugView == DEBUG_VIEW_PRIMARY_RAY_AND_UNORDERED_INTERACTIONS)
  {
    accumulateInDebugViewAdd(geometryResolverState.pixelCoordinate, numUnorderedInteractions);
  }

  bool originalContinueResolving = geometryResolverState.continueResolving;

  // Invoke the Resolve Vertex function

  Ray ray;
  RayInteraction rayInteraction;
  Surface surface;
  SurfaceInteraction surfaceInteraction;
  PolymorphicSurfaceMaterialInteraction polymorphicSurfaceMaterialInteraction;
  
  bool isRaytracedRenderTarget = false;
  bool isStochasticAlphaBlend = false;
  uint8_t firstRayPortal = invalidRayPortalIndex;
  
  resolveVertex<GeometryResolverState>(
    resolveMode,
    rayHitInfo, geometryResolverState,
    ray, rayInteraction,
    surface, surfaceInteraction,
    polymorphicSurfaceMaterialInteraction,
    radianceAttenuation, emissiveRadiance,
    decalMemory, decalEmissiveRadiance, 
    float16_t(cb.screenSpacePixelSpreadHalfAngle),
    isStochasticAlphaBlend, firstRayPortal,
    geometryResolverState.pomOpaqueSurfaceEncountered,
    isRaytracedRenderTarget,
    false, true);
  
  if (cb.startInMediumMaterialIndex != BINDING_INDEX_INVALID)
  {
    const MemoryPolymorphicSurfaceMaterial memoryPolymorphicSurfaceMaterial = surfaceMaterials[cb.startInMediumMaterialIndex];
    const TranslucentSurfaceMaterial translucentSurfaceMaterial = translucentSurfaceMaterialCreate(memoryPolymorphicSurfaceMaterial);

    // Calculate the volume transmittance

    f16vec3 volumeTransmittance;

    if (rayInteractionHasHit(rayInteraction))
    {
      volumeTransmittance = translucentSurfaceMaterialEvalVolumeTransmittance(
        translucentSurfaceMaterial, rayInteraction.hitDistance);
    }
    else
    {
      // Note: Assume infinite hit distance on miss. This wouldn't be true with things that have a practical tMax
      // value (such as NEE rays), but for the geometry resolver this is a fine assumption.
      volumeTransmittance = translucentSurfaceMaterialEvalInfiniteVolumeTransmittance(
        translucentSurfaceMaterial);
    }

    geometryResolverState.attenuation *= volumeTransmittance;

  }

  if (cb.outputParticleLayer && !ignoreTransparencyLayer)
  {
    if (polymorphicSurfaceMaterialInteractionGetTypeHelper(polymorphicSurfaceMaterialInteraction) == surfaceMaterialTypeOpaque && geometryResolverState.continueResolving)
    {
      particleEmissive = emissiveRadiance;
    }
    accumulateParticleBuffer(geometryResolverState.pixelCoordinate, particleEmissive * geometryResolverState.attenuation);
    emissiveRadiance -= particleEmissive;
  }

  bool isUnorderedEmissive = any(emissiveRadiance > 0.0f);

  // Accumulate Segment Hit Distance
  // Note: Only accumulate hit distance if the current ray interaction was not a miss.

  if (rayInteractionHasHit(rayInteraction))
  {
    geometryResolverState.accumulatedHitDistance += geometryResolverState.segmentHitDistance;

#ifndef GBUFFER_PSR
    // Object picking for first resolveVertex hit
    if (numUnorderedInteractions == 0 && cb.enableObjectPicking != 0)
    {
      // Write only if was OBJECT_PICKING_INVALID, i.e. write the very first resolveVertex
      // Object picking for decals
      InterlockedCompareStore(
        PrimaryObjectPicking[geometryResolverState.pixelCoordinate],
        OBJECT_PICKING_INVALID,    // compare to
        surface.objectPickingValue // value to write, if equal
      );
    }
#endif
  }

  // Handle attenuation and emissive radiance from the Resolve function

  // Note: Emissive radiance handled first so objects with opacity do not attenuate themselves.
  geometryResolverState.radiance += emissiveRadiance * geometryResolverState.attenuation;
  geometryResolverState.attenuation *= radianceAttenuation;

  // Record alpha blend surface when stochastic alpha blend is enabled
  if (cb.enableStochasticAlphaBlend && isStochasticAlphaBlend)
  {
    f16vec4 alphaBlendColor = AlphaBlendSurface::getAlphaBlendSurface(polymorphicSurfaceMaterialInteraction);
    bool hasEmissive = any(geometryResolverState.radiance > 0);
    
    AlphaBlendSurface alphaBlendSurface = AlphaBlendSurface.createFromPacked(AlphaBlendGBuffer[geometryResolverState.pixelCoordinate]);
    alphaBlendSurface.update(
      alphaBlendColor, polymorphicSurfaceMaterialInteraction.shadingNormal, geometryResolverState.accumulatedHitDistance, surface.hashPacked, hasEmissive);
    AlphaBlendGBuffer[geometryResolverState.pixelCoordinate] = alphaBlendSurface.pack();

    if (alphaBlendSurface.backgroundTransparency < 1.0 / 255.0)
    {
      geometryResolverState.continueResolving = false;
    }
  }

  if (cb.forceFirstHitInGBufferPass)
  {
    geometryResolverState.continueResolving = false;
  }

  // Check if resolving should continue, and if a surface was hit

  if (geometryResolverState.continueResolving)
  {
    return;
  }

  // Output a miss
  if (!rayInteractionHasHit(rayInteraction))
  {
    updateNrcOnGBufferMiss(geometryResolverState.pixelCoordinate, cb.nrcArgs);

    // Note: Always write to primary surface for initial misses, 
    // it always will be the selected integrated surface in this case as well.
    geometryResolverOutputMiss(true, true, geometryResolverState.pixelCoordinate);
    
    SharedRadianceRG[geometryResolverState.pixelCoordinate] = geometryResolverState.radiance.rg;
    SharedRadianceB[geometryResolverState.pixelCoordinate] = geometryResolverState.radiance.b;
    PrimaryScreenSpaceMotionVectorDLSSRR[geometryResolverState.pixelCoordinate] = calcMotionVectorForRayMiss(geometryResolverState.pixelCoordinate);
    PrimaryDepthDLSSRR[geometryResolverState.pixelCoordinate] = cb.clearColorDepth;

    return;
  }
  
  // Construct RNG

  RNG randomState = createRNG(geometryResolverState.pixelCoordinate, cb.frameIdx);

  // Get PSR related flags and early out of PSR sampling

  const bool enablePSRR = cb.enablePSRR;
  const bool enablePSTR = cb.enablePSTR;
  // pomEnablePSR defaults to false, as PSR does not account for POM, and regular bounce rays will currently give a more accurate result.
  const bool pomEnablePSR = cb.pomEnablePSR;
  // Note: Only sample from PSR if either PSRR or PSTR are enabled, the hit is not a separately traced viewmodel, and the surface does not have displacement. 
  const bool samplePSR =
    (enablePSRR || enablePSTR) &&
    !geometryResolverState.decalEncountered &&
    (pomEnablePSR || !geometryResolverState.pomOpaqueSurfaceEncountered);
  
  // Check if the first hit surface is eligible for PSRR or PSTR by sampling it

  SurfaceMaterialInteractionPSRSample surfaceMaterialInteractionReflectionPSRSample;
  SurfaceMaterialInteractionPSRSample surfaceMaterialInteractionTransmissionPSRSample;
  // Note: Set to 0 in case the weight is not set so will be no diffuse layer contribution.
  f16vec3 diffuseLayerWeight = f16vec3(0.h);
  // Note: Set to 1.0 so if PSR is not sampled 1 will be written out for the integrator to read. We could probably avoid
  // writing this value out in cases we know the PDF will be 1 (any case where only PSRR or PSTR happen but not both) to
  // save some writes/reads, but this should be fine for now.
  float16_t selectedIntegrationSurfacePdf = 1.h;
  const bool oldInsideMedium = geometryResolverState.insideMedium;
  bool pstrPenetrateSurface = false;
  
  if (samplePSR)
  {
    polymorphicSurfaceMaterialInteractionCalcPSRSample(
      polymorphicSurfaceMaterialInteraction, randomState, rayInteraction,
      surfaceMaterialInteractionReflectionPSRSample, surfaceMaterialInteractionTransmissionPSRSample,
      diffuseLayerWeight, geometryResolverState.reflectionSelectedIntegrationSurface, selectedIntegrationSurfacePdf,
      geometryResolverState.insideMedium, pstrPenetrateSurface);

    geometryResolverState.performPSRR =
      enablePSRR &&
      surfaceMaterialInteractionReflectionPSRSample.performPSR;
    geometryResolverState.performPSTR =
      enablePSTR &&
      surfaceMaterialInteractionTransmissionPSRSample.performPSR;
  }

  // GBuffer surface radiance - emissive + diffuse layer (i.e. all except Integrate Direct and Indirect Lighting)
  vec3 gbufferSurfaceRadiance = vec3(0);

  // Add non-direct lighting radiance contributions at the GBuffer hit
  {
    // Add in Emissive contribution

    const vec3 materialEmissiveRadiance = polymorphicSurfaceMaterialInteractionEvalEmissiveRadiance( polymorphicSurfaceMaterialInteraction);

    gbufferSurfaceRadiance += materialEmissiveRadiance;

    // For emissive surface that has sprite sheet animation, put emissive component to particle layer to reduce ghosting
    if (cb.outputParticleLayer && any(materialEmissiveRadiance > 0.0) &&
        surface.spriteSheetRows * surface.spriteSheetCols > 1)
    {
      accumulateParticleBuffer(geometryResolverState.pixelCoordinate, materialEmissiveRadiance * geometryResolverState.attenuation);
    }
    else
    {
      geometryResolverState.radiance += materialEmissiveRadiance * geometryResolverState.attenuation;
    }

    // Approximate the diffuse layer contribution

    // Note: Using attenuation here to factor in the "standard" attenuation from resolving beforehand. Importantly this
    // code needs to come before the PSRR attenuation is factored in otherwise the reflection attenuation will improperly modulate the
    // incident diffuse layer contribution.
    vec3 diffuseLayerRadiance = approximateDiffuseLayerRadiance(
      surfaceInteraction, polymorphicSurfaceMaterialInteraction.shadingNormal, geometryResolverState.portalSpace, diffuseLayerWeight);

    gbufferSurfaceRadiance += diffuseLayerRadiance;

    diffuseLayerRadiance *= geometryResolverState.attenuation;

    // Output diffuse component from the glass to a separate particle layer so that it won't get mixed with background noise.
    if (cb.outputParticleLayer && any(diffuseLayerRadiance > 0.0))
    {
      accumulateParticleBuffer(geometryResolverState.pixelCoordinate, diffuseLayerRadiance);
    }
    else
    {
      geometryResolverState.radiance += diffuseLayerRadiance;
    }
  }

  // Update NRC when a final GBuffer hit occurs.
  // Note: any path termination handling is delayed to indirect pass
  if (!(geometryResolverState.performPSRR || geometryResolverState.performPSTR))
  {
    // Note: radiance and throughput gets reset by NRC for training paths
    updateNrcOnGBufferHitTrainingAndQueryPaths(geometryResolverState.pixelCoordinate,
      geometryResolverState.direction, geometryResolverState.accumulatedHitDistance,
      surfaceInteraction, polymorphicSurfaceMaterialInteraction, gbufferSurfaceRadiance, randomState);
  }

  // Set the PSTR material medium index if a medium was entered
  // Note: This does not work for nested translucency as most recent medium entered is stored, but this is a fine thing to not support as
  // it is not super common. Additionally this path should only be invoked if PSTR is desired, and only in the case of actual thick translucency
  // which changes the medium flag.

  const bool enteredMedium = !oldInsideMedium && geometryResolverState.insideMedium;
  const bool exitedMedium = oldInsideMedium && !geometryResolverState.insideMedium;
  uint16_t transmissionMediumMaterialIndex = cb.startInMediumMaterialIndex;

  if (enteredMedium)
  {
    // Note: For now we always know the only type of material which can trigger this path is the translucent material, so no need to check it.
    const TranslucentSurfaceMaterialInteraction translucentSurfaceMaterialInteraction = translucentSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction);

    // Note: Only setting this for PSTR as it's the only path currently which would need this information (PSRR doesn't off the first interaction at least, but does later
    // during PSR resolving due to things like TIR).
    transmissionMediumMaterialIndex = translucentSurfaceMaterialInteraction.sourceSurfaceMaterialIndex;
  }
  else if (exitedMedium)
  {
    transmissionMediumMaterialIndex = BINDING_INDEX_INVALID;
  }

  if (rayInteraction.customIndex & CUSTOM_INDEX_IS_VIEW_MODEL)
  {
    geometryResolverState.isViewModelSurface = true;
  }

  if (cb.enableRaytracedRenderTarget && isRaytracedRenderTarget)
  {
    GeometryFlags geometryFlags = (GeometryFlags)0;
    geometryFlags.isViewModel = geometryResolverState.isViewModelSurface;
    geometryFlags.portalSpace = geometryResolverState.portalSpace;
    geometryFlags.secondarySurfaceMask = true;
    geometryFlags.objectMask = primaryRayMaskToObjectMask(geometryResolverState.rayMask, geometryResolverState.isViewModelSurface);
    const uint packedGeometryFlags = geometryFlagsEncode(geometryFlags);
    
    const float stochasticTheshold = 0.66f;
    if (getNextSampleBlueNoise(randomState) < stochasticTheshold)
    {
      selectedIntegrationSurfacePdf = stochasticTheshold;
      geometryResolverState.reflectionSelectedIntegrationSurface = false;
    }
    else
    {
      selectedIntegrationSurfacePdf = (1.f - stochasticTheshold);
      geometryResolverState.reflectionSelectedIntegrationSurface = true;
    }

    // render target ray (replacing reflection)
    {
      geometryResolverState.performPSTR = true;
      Ray newRay = rayCreateRenderTarget(cb.renderTargetCamera, surfaceInteraction.textureCoordinates, ray);

      const GbufferPSRData psrData = packPSRData(newRay, BINDING_INDEX_INVALID,
        geometryResolverState.attenuation,
        getIdentityQuaternion(), // TODO[REMIX-3500] this will lead to incorrect motion vectors. It needs to be a full perspective matrix, which won't fit into PSR
        packedGeometryFlags);
        
      storeTransmissionPSRData(psrData, geometryResolverState.pixelCoordinate);
      PrimaryConeRadius[geometryResolverState.pixelCoordinate] = newRay.coneRadius;
      PrimaryHitDistance[geometryResolverState.pixelCoordinate] = geometryResolverState.accumulatedHitDistance;
    }

    // ray that passes through the screen (replacing transmission)
    {
      geometryResolverState.performPSRR = true;
      Ray newRay = rayCreateDirection(
        rayInteraction, surfaceInteraction, ray, ray.direction, true);
      const GbufferPSRData psrData = packPSRData(newRay, BINDING_INDEX_INVALID,
        geometryResolverState.attenuation,
        geometryResolverState.accumulatedRotation,
        packedGeometryFlags);
        
      storeReflectionPSRData(psrData, geometryResolverState.pixelCoordinate);
      SecondaryHitDistance[geometryResolverState.pixelCoordinate] = geometryResolverState.accumulatedHitDistance;
    }
  }
  else if (geometryResolverState.performPSRR || geometryResolverState.performPSTR)
  {
    // Set the geometry flag bits that must be the same for both PSR passes, see unpackPSRData(...)

    GeometryFlags geometryFlags = (GeometryFlags)0;
    geometryFlags.isViewModel = geometryResolverState.isViewModelSurface;
    geometryFlags.portalSpace = geometryResolverState.portalSpace;
    geometryFlags.pomOpaqueSurfaceEncountered = geometryResolverState.pomOpaqueSurfaceEncountered;
    geometryFlags.objectMask = primaryRayMaskToObjectMask(geometryResolverState.rayMask, geometryResolverState.isViewModelSurface);
    const uint packedGeometryFlags = geometryFlagsEncode(geometryFlags);

    // Set Reflection PSR information

    if (geometryResolverState.performPSRR)
    {
      // Todo: Put this direction sampling and state logic into a function similar to how the integrator does it

      // Create a ray from the Reflection PSR material sample

      // Note: Penetrate surface flag always known to be false for reflection events.
      const Ray sampledRay = rayCreateDirection(
        rayInteraction, surfaceInteraction, ray, surfaceMaterialInteractionReflectionPSRSample.inputDirection, false);

      f16vec4 vectorTransform = quaternionMultiply(geometryResolverState.accumulatedRotation, surfaceMaterialInteractionReflectionPSRSample.vectorTransform, true);
      
      const GbufferPSRData psrData = packPSRData(sampledRay, BINDING_INDEX_INVALID,
        geometryResolverState.attenuation * surfaceMaterialInteractionReflectionPSRSample.attenuation,
        vectorTransform,
        packedGeometryFlags);

      storeReflectionPSRData(psrData, geometryResolverState.pixelCoordinate);

      geometryResolverState.coneRadius = sampledRay.coneRadius;

      // Note: Set the common alternate disocclusion threshold flag if reflection PSR requests it.
      if (surfaceMaterialInteractionReflectionPSRSample.useAlternateDisocclusionThreshold)
      {
        geometryResolverState.useAlternateDisocclusionThreshold = true;
      }
    }

    // Set Transmission PSR information

    if (geometryResolverState.performPSTR)
    {
      // Create a ray from the Transmission PSR material sample

      const Ray sampledRay = rayCreateDirection(
        rayInteraction, surfaceInteraction, ray, surfaceMaterialInteractionTransmissionPSRSample.inputDirection, pstrPenetrateSurface);

      f16vec4 vectorTransform = quaternionMultiply(geometryResolverState.accumulatedRotation, surfaceMaterialInteractionTransmissionPSRSample.vectorTransform, true);
      
      const GbufferPSRData psrData = packPSRData(sampledRay, transmissionMediumMaterialIndex,
        geometryResolverState.attenuation * surfaceMaterialInteractionTransmissionPSRSample.attenuation,
        vectorTransform,
        packedGeometryFlags);

      storeTransmissionPSRData(psrData, geometryResolverState.pixelCoordinate);

      // Note: Overwriting cone radius here if both PSRR and PSTR are active, but this should be fine as the value will
      // be identical in both cases.
      geometryResolverState.coneRadius = sampledRay.coneRadius;

      // Note: Set the common alternate disocclusion threshold flag if transmission PSR requests it.
      if (surfaceMaterialInteractionTransmissionPSRSample.useAlternateDisocclusionThreshold)
      {
        geometryResolverState.useAlternateDisocclusionThreshold = true;
      }
    }

    PrimaryConeRadius[geometryResolverState.pixelCoordinate] = rayInteraction.coneRadius;
    PrimaryHitDistance[geometryResolverState.pixelCoordinate] = geometryResolverState.accumulatedHitDistance;
    SecondaryHitDistance[geometryResolverState.pixelCoordinate] = geometryResolverState.accumulatedHitDistance;

    // The actual threshold value will be set when storing gbuffer data for the PSR resolved primary surface.
    // Right now we just set the flag to carry over alternate threshold being set to the PSR pass.
    PrimaryDisocclusionThresholdMix[geometryResolverState.pixelCoordinate] = geometryResolverState.useAlternateDisocclusionThreshold;
  }
  else
  {
    // Output the Surface if no form of PSR should happen (Standard surface case)

    // Note: Consider this surface the primary surface and the surface to invoke the integrator on when no PSR is done.
    geometryResolverOutputSurface(
      true, true, geometryResolverState.pixelCoordinate,
      ray, rayInteraction,
      surface, surfaceInteraction,
      polymorphicSurfaceMaterialInteraction,
      geometryResolverState.radiance, geometryResolverState.attenuation,
      geometryResolverState.accumulatedRotation, cb.startInMediumMaterialIndex,
      geometryResolverState.accumulatedHitDistance, geometryResolverState.directionAltered,
      geometryResolverState.useAlternateDisocclusionThreshold,
      geometryResolverState.isViewModelSurface);
  }
  
  // Output Emissive Radiance
  SharedRadianceRG[geometryResolverState.pixelCoordinate] = geometryResolverState.radiance.rg;
  SharedRadianceB[geometryResolverState.pixelCoordinate] = geometryResolverState.radiance.b;

  // The particles (shared bias mask) are using opaque approximations, which only result in monochromatic attenuation, so we just need to output single channel here
  SharedBiasCurrentColorMask[geometryResolverState.pixelCoordinate] = isUnorderedEmissive ? 1.0f : 0.0f;

  // Output the Integration Surface PDF
  // Note: This is written out so the integrator can use this as a starting point due to basing itself off this stochastic choice
  // if PSR is used. Note this does not need to apply to anything within the g-buffer pass as the PSR continuation logic is non-stochastic
  // meaning both paths will be evaluated fully without skipping data like the integrator will due to only being able to chose one of the
  // potential two paths.

  SharedIntegrationSurfacePdf[geometryResolverState.pixelCoordinate] = selectedIntegrationSurfacePdf;

  // Store DLSSRR data
  if (cb.enableDLSSRR)
  {
    geometryResolverDLSSRROutputSurface(
        geometryResolverState.pixelCoordinate, ray, surface, surfaceInteraction, polymorphicSurfaceMaterialInteraction,
        geometryResolverState.attenuation, geometryResolverState.accumulatedRotation, geometryResolverState.accumulatedHitDistance
    );
  }

  // Output Debug View
  // Note: This is always done on the first hit rather than the resolved "primary" hits due to the branching potential.

  geometryResolverVertexOutputDebugView(
    geometryResolverState.pixelCoordinate,
    ray, rayInteraction,
    surface, surfaceInteraction,
    polymorphicSurfaceMaterialInteraction,
    selectedIntegrationSurfacePdf,
    geometryResolverState.directionAltered, geometryResolverState.portalSpace,
    geometryResolverState.accumulatedHitDistance,
    isRaytracedRenderTarget);
}


bool isRefractionDistorted(GeometryPSRResolverState geometryPSRResolverState, SurfaceInteraction surfaceInteraction)
{
  // Check if the current surface point's screen space projection position is significantly different from the pixel position.
  const uvec2 pixelCoordinate = geometryPSRResolverState.pixelCoordinate;
  const vec4 virtualPosScreen = mul(cb.camera.worldToProjectionJittered, vec4(surfaceInteraction.position, 1.f));
  const vec2 screenMul = vec2(cb.camera.resolution.x * 0.5f, cb.camera.resolution.y * -0.5f);
  const vec2 posScreen = screenMul * (virtualPosScreen.xy / virtualPosScreen.w) + vec2(cb.camera.resolution.xy) * 0.5f;
  const vec2 posScreenDirect = vec2(geometryPSRResolverState.pixelCoordinate) + 0.5f;

  const vec2 diff = abs(posScreenDirect - posScreen);
  const float sqRefractionPixelErrorThreshold = 8.f;
  return dot(diff, diff) >= sqRefractionPixelErrorThreshold;
}

// Note: Logic grouped into function to make loop count termination more unified with non-PSR continuation cases in calling logic.
void geometryPSRResolverEvaluateContinuation(
  inout GeometryPSRResolverState geometryPSRResolverState, uint8_t psrType,
  MinimalRayInteraction minimalRayInteraction, PolymorphicSurfaceMaterialInteraction polymorphicSurfaceMaterialInteraction,
  inout SurfaceMaterialInteractionPSRSample surfaceMaterialInteractionReflectionPSRSample,
  inout SurfaceMaterialInteractionPSRSample surfaceMaterialInteractionTransmissionPSRSample,
  inout f16vec3 diffuseLayerWeight,
  inout bool penetrateSurface,
  inout bool useReflectionPSRSample,
  inout bool useTransmissionPSRSample,
  inout bool isLastBounce)
{
  // Check for PSR Resolver termination

  bool shouldTerminate = false;
  isLastBounce = false;

  if (psrType == psrTypeReflection)
  {
    shouldTerminate = geometryPSRResolverState.bounceIteration > cb.psrrMaxBounces;
    isLastBounce = geometryPSRResolverState.bounceIteration >= cb.psrrMaxBounces;
  }
  else if (psrType == psrTypeTransmission)
  {
    shouldTerminate = geometryPSRResolverState.bounceIteration > cb.pstrMaxBounces;
    isLastBounce = geometryPSRResolverState.bounceIteration >= cb.pstrMaxBounces;
  }

  // We currently don't support reflections off of POM
  if (!cb.pomEnablePSR && polymorphicSurfaceMaterialInteractionHasHeightTexture(polymorphicSurfaceMaterialInteraction))
  {
    shouldTerminate = true;
  }

  if (shouldTerminate)
  {
    useReflectionPSRSample = false;
    useTransmissionPSRSample = false;

    return;
  }

  // Attempt to sample new PSR information

  // Note: Selected integration surface not relevant when evaluating a PSR path, only used on the first hit. Random state not
  // needed either in this case as the RNG is only used for doing this stochastic selection.
  RNG dummyRandomState = createRNG(uvec2(0, 0), 0);
  bool dummyReflectionSelectedIntegrationSurface;
  float16_t dummySelectedIntegrationSurfacePdf;

  polymorphicSurfaceMaterialInteractionCalcPSRSample(
    polymorphicSurfaceMaterialInteraction, dummyRandomState, minimalRayInteraction,
    surfaceMaterialInteractionReflectionPSRSample, surfaceMaterialInteractionTransmissionPSRSample,
    diffuseLayerWeight, dummyReflectionSelectedIntegrationSurface, dummySelectedIntegrationSurfacePdf,
    geometryPSRResolverState.insideMedium, penetrateSurface);

  // Calculate transmission related PSR flags

  const bool pstrOutgoingSplitApproximation = cb.enablePSTROutgoingSplitApproximation;
  const bool pstrSecondaryIncidentSplitApproximation = cb.enablePSTRSecondaryIncidentSplitApproximation;

  // Table of transmission-related flag cases:
  // Inside Medium | Penetrate Surface | Meaning
  // --------------+-------------------+--------
  // True          | False             | TIR case
  // False         | True              | Inside->Outside Transmission Exception (Decent quality except around TIR borders when enabled)
  // True          | True              | Outside->Inside Transmission Exception (Less high quality in multi-layered transmission when enabled but good for many layers of windows)
  // False         | False             | Impossible state for transmission logic typically, not something that needs to be worried about.
  // Note: Inside medium flag describes the state of the sampled ray, for example if the ray is currently inside a medium and a transmission event is sampled,
  // it will be set to false upon reaching this point.
  const bool tirSampleCase = geometryPSRResolverState.insideMedium && !penetrateSurface;
  const bool insideOutsideTransmissionCase = !geometryPSRResolverState.insideMedium && penetrateSurface && pstrOutgoingSplitApproximation;
  const bool outsideInsideTransmissionCase = geometryPSRResolverState.insideMedium && penetrateSurface && pstrSecondaryIncidentSplitApproximation;

  // Determine which PSR sample information should be used (if any) for PSR continuation

  // Note: PSR continuation follows reflection events when no path splits occur (typical opaque mirrorlike reflection),
  // follows transmission TIR, as well as making an expection on the no splitting rule by following transmission on
  // surface-exiting or entering translucency interactions (when requested). Note this will cause incorrect looking darkening
  // near the TIR boundary on glass as well as missing translucent reflections past the first translucent hit, but it will
  // significantly improve quality when dealing with single glass layers. Additionally note that the first hit where a split
  // occured is already handled on translucent surfaces by a split of reflections and transmission, this logic is only concerned
  // with the continuation path the PSR takes. Finally, note that this logic is the same between if the psrType is reflection
  // or transmission because both Reflection and Transmission PSR follow the same rules essentially, allowing Reflection PSR
  // to follow transmission events just the same when approximations are requested.

  // Note: Reflection and transmission PSR should be mutually exclusive with this setup.
  useReflectionPSRSample =
    surfaceMaterialInteractionReflectionPSRSample.performPSR &&
    !surfaceMaterialInteractionTransmissionPSRSample.performPSR;
  useTransmissionPSRSample =
    surfaceMaterialInteractionTransmissionPSRSample.performPSR &&
    (tirSampleCase || insideOutsideTransmissionCase || outsideInsideTransmissionCase);
}

void geometryPSRResolverVertex(
  EmptyExtraArgs extraArgs, RayHitInfo rayHitInfo,
  inout GeometryPSRResolverState geometryPSRResolverState)
{
  // Setup resolve state
  uint8_t psrType = geometryPSRResolverState.isTransmissionPSR ? psrTypeTransmission : psrTypeReflection;

  // Note: Emissives are fine to approximate even on primary rays (since lighting rarely
  // matters on them), and non-emissive opacity particles should be lit with the lighting approximation.
  const uint8_t resolveMode =
    resolveModeAlteredDirectionNotify |
    resolveModeEmissiveOpacityTransmissionApprox |
    resolveModeDecalMaterialBlending |
    (cb.enableSeparateUnorderedApproximations ? resolveModeSeparateUnorderedApproximations : 0);
  f16vec3 radianceAttenuation = f16vec3(1.0f, 1.0f, 1.0f);
  vec3 emissiveRadiance = vec3(0);

  // Handle Unordered Resolving
  // Note: Done before resolveVertex so attenuation from unordered objects
  // between the origin and the hit point can be accumulated in advance.

  Ray unorderedResolveRay;
  unorderedResolveRay.origin = geometryPSRResolverState.origin;
  unorderedResolveRay.coneRadius = geometryPSRResolverState.coneRadius;
  unorderedResolveRay.spreadAngle = float16_t(cb.screenSpacePixelSpreadHalfAngle);
  unorderedResolveRay.direction = geometryPSRResolverState.direction;
  unorderedResolveRay.tMax = rayHitInfo.hasHit ? rayHitInfo.hitDistance * kUnorderedResolveRayLengthening : floatMax;

  uint numUnorderedInteractions = 0;
  uint8_t unorderedRayMask = convertPrimaryRayMaskToUnordered(geometryPSRResolverState.rayMask, OBJECT_MASK_ALL_UNORDERED);

  geometryPSRResolverState.decalEncountered = false;
  
  uint4 decalMemory = 0;
  f16vec3 decalEmissiveRadiance = f16vec3(0);
  int surfaceIndex;
  resolveVertexUnordered<RAY_FLAG_CULL_OPAQUE>(
    resolveMode, unorderedResolveRay,
    unorderedRayMask, geometryPSRResolverState.portalSpace,
    /* useIntersectionBillboards = */ cb.enableBillboardOrientationCorrection,
    u16vec2(65535, 65535),
    geometryPSRResolverState.accumulatedRotation, geometryPSRResolverState.accumulatedHitDistance, 1.h,
    radianceAttenuation, emissiveRadiance,
    numUnorderedInteractions, 
    geometryPSRResolverState.decalEncountered, decalMemory, decalEmissiveRadiance, surfaceIndex);

  vec4 particleEmissive = vec4(0,0,0,1);
  bool ignoreTransparencyLayer = false;
  if (surfaceIndex >= 0)
  {
    const Surface surface = surfaces[uint(surfaceIndex)];
    ignoreTransparencyLayer = surface.ignoreTransparencyLayer;
  }
  if (cb.outputParticleLayer && any(emissiveRadiance > 0) && !ignoreTransparencyLayer)
  {
    particleEmissive.xyz += emissiveRadiance * geometryPSRResolverState.attenuation;
    emissiveRadiance = 0;
  }

  // Note: Add to the number of unordered interactions in the debug view based on the number of unordered interactions in resolving.
  if (cb.debugView == DEBUG_VIEW_PRIMARY_UNORDERED_INTERACTIONS || cb.debugView == DEBUG_VIEW_PRIMARY_RAY_AND_UNORDERED_INTERACTIONS)
  {
    accumulateInDebugViewAdd(geometryPSRResolverState.pixelCoordinate, numUnorderedInteractions);
  }

  // Invoke the Resolve Vertex function

  Ray ray;
  RayInteraction rayInteraction;
  Surface surface;
  SurfaceInteraction surfaceInteraction;
  PolymorphicSurfaceMaterialInteraction polymorphicSurfaceMaterialInteraction;
  bool isRaytracedRenderTarget = false;

  bool isStochasticAlphaBlend = false; // note, not actually used here
  uint8_t firstRayPortal = invalidRayPortalIndex;
  
  resolveVertex<GeometryPSRResolverState>(
    resolveMode,
    rayHitInfo, geometryPSRResolverState,
    ray, rayInteraction,
    surface, surfaceInteraction,
    polymorphicSurfaceMaterialInteraction,
    radianceAttenuation, emissiveRadiance,
    decalMemory, decalEmissiveRadiance, 
    float16_t(cb.screenSpacePixelSpreadHalfAngle),
    isStochasticAlphaBlend, firstRayPortal,
    geometryPSRResolverState.pomOpaqueSurfaceEncountered,
    isRaytracedRenderTarget);

  // Handle attenuation and emissive radiance from the Resolve function

  // Note: Emissive radiance handled first so objects with opacity do not attenuate themselves.
  if (cb.outputParticleLayer && polymorphicSurfaceMaterialInteractionGetTypeHelper(polymorphicSurfaceMaterialInteraction) == surfaceMaterialTypeOpaque && geometryPSRResolverState.continueResolving)
  {
    particleEmissive.xyz += emissiveRadiance * geometryPSRResolverState.attenuation;
    emissiveRadiance = 0;
  }

  if (any(particleEmissive != vec4(0,0,0,1)))
  {
    accumulateParticleBuffer(geometryPSRResolverState.pixelCoordinate, particleEmissive.xyz);
  }

  // Handle medium attenuation if needed
  // Note: This is done here to avoid needing to accumulate the total resolved hit distance. Slightly
  // more expensive in some cases than doing it after resolving is finished, but saves payload space.
  // Additionally this allows for proper handling of infinite attenuation when a miss happens.

  if (geometryPSRResolverState.mediumMaterialIndex != BINDING_INDEX_INVALID)
  {
    const MemoryPolymorphicSurfaceMaterial memoryPolymorphicSurfaceMaterial = surfaceMaterials[geometryPSRResolverState.mediumMaterialIndex];
    const TranslucentSurfaceMaterial translucentSurfaceMaterial = translucentSurfaceMaterialCreate(memoryPolymorphicSurfaceMaterial);

    // Calculate the volume transmittance

    f16vec3 volumeTransmittance;

    if (rayInteractionHasHit(rayInteraction))
    {
      volumeTransmittance = translucentSurfaceMaterialEvalVolumeTransmittance(
        translucentSurfaceMaterial, rayInteraction.hitDistance);
    }
    else
    {
      // Note: Assume infinite hit distance on miss. This wouldn't be true with things that have a practical tMax
      // value (such as NEE rays), but for the geometry resolver this is a fine assumption.
      volumeTransmittance = translucentSurfaceMaterialEvalInfiniteVolumeTransmittance(
        translucentSurfaceMaterial);
    }

    geometryPSRResolverState.attenuation *= volumeTransmittance;
  }

  // Accumulate Segment Hit Distance
  // Note: Only accumulate hit distance if the current ray interaction was not a miss.

  if (rayInteractionHasHit(rayInteraction))
  {
    geometryPSRResolverState.accumulatedHitDistance += geometryPSRResolverState.segmentHitDistance;
  }
  else
  {
    if (cb.domeLightArgs.active)
    {
       emissiveRadiance += cb.domeLightArgs.radiance * sampleDomeLightTexture(LinearWrapSampler, ray.direction, cb.domeLightArgs.textureIndex, cb.domeLightArgs.worldToLightTransform) * radianceAttenuation;
    }
    else
    {
      // Output radiance from sky probe
       emissiveRadiance += cb.skyBrightness * SkyProbe.SampleLevel(ray.direction, 0) * radianceAttenuation;
    }
  }

  // Note: Emissive radiance handled first so objects with opacity do not attenuate themselves.
  geometryPSRResolverState.radiance += emissiveRadiance * geometryPSRResolverState.attenuation;

  // Check if resolving should continue, and if a surface was hit

  if (geometryPSRResolverState.continueResolving)
  {
    // Compute the pixel error compared to a direct ray from camera    
    uint8_t materialType = polymorphicSurfaceMaterialInteractionGetTypeHelper(polymorphicSurfaceMaterialInteraction);
    if (materialType == surfaceMaterialTypeRayPortal)
    {
      geometryPSRResolverState.isRefractionDistorted = isRefractionDistorted(geometryPSRResolverState, surfaceInteraction);
    }

    return;
  }

  if (!rayInteractionHasHit(rayInteraction))
  {
    // NRC is only used for paths that are fully integrated
    if (geometryPSRResolverState.selectedIntegrationSurface)
    {
      updateNrcOnGBufferMiss(geometryPSRResolverState.pixelCoordinate, cb.nrcArgs);
    }

    // Output Miss
    SharedRadianceRG[geometryPSRResolverState.pixelCoordinate] += geometryPSRResolverState.radiance.rg;
    SharedRadianceB[geometryPSRResolverState.pixelCoordinate] += geometryPSRResolverState.radiance.b;

    geometryResolverOutputMiss(
      geometryPSRResolverState.primarySurface, geometryPSRResolverState.selectedIntegrationSurface,
      geometryPSRResolverState.pixelCoordinate);

    return;
  }

  // Now apply attenuation after dealing with the miss case (where attenuation is 0)
  geometryPSRResolverState.attenuation *= radianceAttenuation;

  // Evaluate PSR continuation logic

  SurfaceMaterialInteractionPSRSample surfaceMaterialInteractionReflectionPSRSample;
  SurfaceMaterialInteractionPSRSample surfaceMaterialInteractionTransmissionPSRSample;
  f16vec3 diffuseLayerWeight = f16vec3(0.0f);
  const bool oldInsideMedium = geometryPSRResolverState.insideMedium;
  bool penetrateSurface = false;
  bool useReflectionPSRSample = false;
  bool useTransmissionPSRSample = false;
  bool isLastBounce = false;

  geometryPSRResolverEvaluateContinuation(
    geometryPSRResolverState, psrType,
    rayInteraction, polymorphicSurfaceMaterialInteraction,
    surfaceMaterialInteractionReflectionPSRSample, surfaceMaterialInteractionTransmissionPSRSample,
    diffuseLayerWeight, penetrateSurface, useReflectionPSRSample, useTransmissionPSRSample, isLastBounce);

  // GBuffer surface radiance - emissive + diffuse layer (i.e. all except Integrate Direct and Indirect Lighting)
  vec3 gbufferSurfaceRadiance = vec3(0);

  // Add non-direct lighting radiance contributions at the GBuffer hit
  {
    // Add in Emissive contribution

    const vec3 materialEmissiveRadiance = polymorphicSurfaceMaterialInteractionEvalEmissiveRadiance(polymorphicSurfaceMaterialInteraction);

    geometryPSRResolverState.radiance += materialEmissiveRadiance * geometryPSRResolverState.attenuation;

    gbufferSurfaceRadiance += materialEmissiveRadiance;

    // Approximate the diffuse layer contribution

    // Note: This code needs to come before the PSR attenuation is factored in otherwise the reflection or transmission attenuation will improperly
    // modulate the diffuse layer contribution.
    vec3 diffuseLayerRadiance = approximateDiffuseLayerRadiance(
      surfaceInteraction, polymorphicSurfaceMaterialInteraction.shadingNormal, geometryPSRResolverState.portalSpace, diffuseLayerWeight);

    gbufferSurfaceRadiance += diffuseLayerRadiance;

    diffuseLayerRadiance *= geometryPSRResolverState.attenuation;

    if (cb.outputParticleLayer)
    {
      accumulateParticleBuffer(geometryPSRResolverState.pixelCoordinate, diffuseLayerRadiance);
    }
    else
    {
      geometryPSRResolverState.radiance += diffuseLayerRadiance;
    }
  }

  // Update NRC when a final GBuffer hit occurs.
  // Note: any path termination handling is delayed to indirect pass
  if (!(useReflectionPSRSample || useTransmissionPSRSample))
  {
    // NRC is only used for paths that are fully integrated
    if (geometryPSRResolverState.selectedIntegrationSurface)
    {
      RNG randomState = createRNG(geometryPSRResolverState.pixelCoordinate, cb.frameIdx);

      // Note: radiance and throughput gets reset by NRC for training paths
      updateNrcOnGBufferHitTrainingAndQueryPaths(geometryPSRResolverState.pixelCoordinate,
        geometryPSRResolverState.direction, geometryPSRResolverState.accumulatedHitDistance, 
        surfaceInteraction, polymorphicSurfaceMaterialInteraction, gbufferSurfaceRadiance, randomState);
    }
  } 

  // Set the material medium index if a medium was entered or exited
  // Note: This does not work for nested translucency as most recent medium entered is stored, but this is a fine thing to not support as
  // it is not super common. Additionally this path should only be invoked if PSTR is desired, and only in the case of actual thick translucency
  // which changes the medium flag.

  const bool enteredMedium = !oldInsideMedium && geometryPSRResolverState.insideMedium;
  const bool exitedMedium = oldInsideMedium && !geometryPSRResolverState.insideMedium;

  if (enteredMedium)
  {
    // Note: For now we always know the only type of material which can trigger this path is the translucent material, so no need to check it.
    const TranslucentSurfaceMaterialInteraction translucentSurfaceMaterialInteraction = translucentSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction);

    geometryPSRResolverState.mediumMaterialIndex = translucentSurfaceMaterialInteraction.sourceSurfaceMaterialIndex;
  }
  else if (exitedMedium)
  {
    geometryPSRResolverState.mediumMaterialIndex = BINDING_INDEX_INVALID;
  }

  // Handle PSR continuation or output a surface otherwise

  // If the camera index changed then create a new ray and return
  if (cb.enableRaytracedRenderTarget && isRaytracedRenderTarget && geometryPSRResolverState.bounceIteration < cb.psrrMaxBounces)
  {
    Ray newRay = rayCreateRenderTarget(cb.renderTargetCamera, surfaceInteraction.textureCoordinates, ray);
    geometryPSRResolverState.origin = newRay.origin;
    geometryPSRResolverState.direction = newRay.direction;
    geometryPSRResolverState.coneRadius = newRay.coneRadius;
    geometryPSRResolverState.continuePSRResolving = true;
    geometryPSRResolverState.insideMedium = false;
    geometryPSRResolverState.mediumMaterialIndex = cb.startInMediumMaterialIndex;
    geometryPSRResolverState.accumulatedRotation = getIdentityQuaternion();
    
  }
  else if (useReflectionPSRSample)
  {
    // Set PSR resolver state information from the reflection sample for the next iteration

    // Note: Penetrate surface flag always known to be false for reflection events.
    const Ray sampledRay = rayCreateDirection(
      rayInteraction, surfaceInteraction, ray, surfaceMaterialInteractionReflectionPSRSample.inputDirection, false);

    geometryPSRResolverState.origin = sampledRay.origin;
    geometryPSRResolverState.coneRadius = sampledRay.coneRadius;
    geometryPSRResolverState.direction = sampledRay.direction;
    geometryPSRResolverState.attenuation *= surfaceMaterialInteractionReflectionPSRSample.attenuation;
    geometryPSRResolverState.continuePSRResolving = true;
    geometryPSRResolverState.accumulatedRotation =
      quaternionMultiply(geometryPSRResolverState.accumulatedRotation, surfaceMaterialInteractionReflectionPSRSample.vectorTransform, true);
      
    // Note: Set the common alternate disocclusion threshold flag if reflection PSR requests it.
    if (surfaceMaterialInteractionReflectionPSRSample.useAlternateDisocclusionThreshold)
    {
      geometryPSRResolverState.useAlternateDisocclusionThreshold = true;
    }
  }
  else if (useTransmissionPSRSample)
  {
    // Set PSR resolver state information from the transmission sample for the next iteration

    const Ray sampledRay = rayCreateDirection(
      rayInteraction, surfaceInteraction, ray, surfaceMaterialInteractionTransmissionPSRSample.inputDirection, penetrateSurface);

    geometryPSRResolverState.origin = sampledRay.origin;
    geometryPSRResolverState.coneRadius = sampledRay.coneRadius;
    geometryPSRResolverState.direction = sampledRay.direction;
    geometryPSRResolverState.attenuation *= surfaceMaterialInteractionTransmissionPSRSample.attenuation;
    geometryPSRResolverState.continuePSRResolving = true;
    geometryPSRResolverState.accumulatedRotation =
      quaternionMultiply(geometryPSRResolverState.accumulatedRotation, surfaceMaterialInteractionTransmissionPSRSample.vectorTransform, true);

    // Note: Set the common alternate disocclusion threshold flag if transmission PSR requests it.
    if (surfaceMaterialInteractionTransmissionPSRSample.useAlternateDisocclusionThreshold)
    {
      geometryPSRResolverState.useAlternateDisocclusionThreshold = true;
    }
  }
  else // Note: This case happens either when no PSR continuation is desired or if the max PSR bounce count was hit.
  {
    // Output the resolved Surface

    // Note: directionAltered set to true as PSR will almost always alter the ray direction.
    geometryResolverOutputSurface(
      geometryPSRResolverState.primarySurface, geometryPSRResolverState.selectedIntegrationSurface,
      geometryPSRResolverState.pixelCoordinate,
      ray, rayInteraction,
      surface, surfaceInteraction,
      polymorphicSurfaceMaterialInteraction,
      geometryPSRResolverState.radiance, geometryPSRResolverState.attenuation,
      geometryPSRResolverState.accumulatedRotation, geometryPSRResolverState.mediumMaterialIndex,
      geometryPSRResolverState.accumulatedHitDistance, true,
      geometryPSRResolverState.useAlternateDisocclusionThreshold,
      geometryPSRResolverState.isViewModelSurface);

    // Output Emissive Radiance

    SharedRadianceRG[geometryPSRResolverState.pixelCoordinate] = geometryPSRResolverState.radiance.rg;
    SharedRadianceB[geometryPSRResolverState.pixelCoordinate] = geometryPSRResolverState.radiance.b;
  }

  if (rayInteraction.customIndex & CUSTOM_INDEX_IS_VIEW_MODEL)
  {
    geometryPSRResolverState.isViewModelSurface = true;
  }

  // Replace DLSS-RR motion vectors for transparent objects if screen-space refraction error is not too large.
  // This means that we replace the motion vectors (and depth + normals) until error is too large, giving us the
  // last interaction that's usable for motion vectors.
  if (cb.enableDLSSRR && psrType == psrTypeTransmission)
  {
    // Compute the pixel error compared to a direct ray from camera
    const uvec2 pixelCoordinate = geometryPSRResolverState.pixelCoordinate;
    bool hasRefractionThis = isRefractionDistorted(geometryPSRResolverState, surfaceInteraction);
    uint8_t materialType = polymorphicSurfaceMaterialInteractionGetTypeHelper(polymorphicSurfaceMaterialInteraction);
    bool hasPortalThis = geometryPSRResolverState.portalSpace != PORTAL_SPACE_NONE;
    const float16_t minAttenuation = 0.2;

    if (calcBt709Luminance(geometryPSRResolverState.attenuation) > minAttenuation &&
        (!hasRefractionThis || (materialType != surfaceMaterialTypeRayPortal && hasPortalThis)) &&
        !geometryPSRResolverState.isRefractionDistorted)
    {
      // Recreate the original primary camera ray
      const Ray originalCameraRay = rayCreatePrimaryFromPixel(cb.camera, pixelCoordinate, false);

      // Note: Using virtual hit distance accumulated through PSR and Ray Portals used to calculate a virtual hit position
      const vec3 virtualHitPosition = rayEvaluate(originalCameraRay, geometryPSRResolverState.accumulatedHitDistance);
      const float4 projectionSpacePosition = mul(cb.camera.worldToProjectionJittered, float4(virtualHitPosition, 1.f));
      const float depth = projectionSpacePosition.z / projectionSpacePosition.w;
      imageStore(PrimaryDepthDLSSRR, pixelCoordinate, depth);

      // Output Motion Vector (Primary/secondary surface coherent)
      // Note: Virtual motion used to account for the ray traversing ray portals and through PSR interactions.
      vec3 virtualMotion = quaternionTransformVector(geometryPSRResolverState.accumulatedRotation, surfaceInteraction.motion, true);
      
      vec4 prevWorldPosition = vec4(virtualHitPosition + virtualMotion, 1);
      vec4 prevNDC = mul(cb.camera.prevWorldToProjection, prevWorldPosition);
      prevNDC /= prevNDC.w;

      vec2 ndc = cameraPixelCoordinateToNDC(cb.camera, pixelCoordinate);
      vec2 motionVectorNDC = prevNDC.xy - ndc.xy;
      vec2 motionVectorUVOffset = motionVectorNDC * vec2(0.5, -0.5);
      vec2 motionVectorPixel = motionVectorUVOffset * cb.camera.resolution;

      // Write out screen-space motion.
      PrimaryScreenSpaceMotionVectorDLSSRR[pixelCoordinate] = motionVectorPixel;

      const GBufferMemoryPolymorphicSurfaceMaterialInteraction gBufferMemoryPolymorphicSurfaceMaterialInteraction =
          gBufferMemoryPolymorphicSurfaceMaterialInteractionCreate(polymorphicSurfaceMaterialInteraction);

      // Write out shading normal
      PrimaryWorldShadingNormalDLSSRR[pixelCoordinate] = vec4(gBufferMemoryPolymorphicSurfaceMaterialInteraction.worldShadingNormal, 0);
    }

    if (hasRefractionThis && (materialType == surfaceMaterialTypeOpaque || materialType == surfaceMaterialTypeRayPortal))
    {
      geometryPSRResolverState.isRefractionDistorted = true;
    }
  }

  // If this is the last bounce, ignore translucent geometry (e.g. glass)
  // so that we don't end up with a translucent surface in the G-buffer.
  // That results in ReSTIR GI not working on some parts of detailed glass objects,
  // and therefore producing excessive noise.
  if (isLastBounce)
  {
    geometryPSRResolverState.rayMask &= ~OBJECT_MASK_TRANSLUCENT;
  }
}

void preprocessPrimaryRayState(inout Ray originalCameraRay, inout GeometryResolverState geometryResolverState)
{
  // Recreate the original primary camera ray

  originalCameraRay = rayCreatePrimaryFromPixel(cb.camera, geometryResolverState.pixelCoordinate);

  // Determine the ray mask

  uint8_t rayMask = 
      OBJECT_MASK_ALL | 
      OBJECT_MASK_VIEWMODEL |
      OBJECT_MASK_PLAYER_MODEL_VIRTUAL;

  if (cb.enablePlayerModelInPrimarySpace)
  {
    rayMask |= OBJECT_MASK_PLAYER_MODEL;
  }

  if (cb.startInMediumMaterialIndex != BINDING_INDEX_INVALID) {
    geometryResolverState.insideMedium = true;
  }

  geometryResolverState.rayMask = rayMask;
}

uint8_t primaryRayMaskToObjectMask(uint8_t rayMask, bool isViewModel)
{
  if (isViewModel)
  {
    rayMask &= ~OBJECT_MASK_ALL_PLAYER_MODEL;
  }
  else
  {
    rayMask = (rayMask & ~OBJECT_MASK_ALL_VIEWMODEL);

    if (cb.enablePlayerModelPrimaryShadows)
    {
      rayMask = rayMask | OBJECT_MASK_ALL_PLAYER_MODEL;
    }
  }

  return rayMask;
}

// Geometry Resolver Functions

void geometryPSRResolver(inout GeometryPSRResolverState psrResolverState) {
  for (;;)
  {
    // Set iteration PSR Resolver State

    ++psrResolverState.bounceIteration;
    // Note: Set to false to be set to true only if the resolver should continue in the PSR resolver function.
    // This is done to easily account for the various cases the path vertex function may return on
    // which do not require path extension.
    psrResolverState.continuePSRResolving = false;

    // Create a secondary Ray from the PSR Resolver State

    // Todo: Find a better way to construct this, ideally a
    // version without tMax when we have generic functions,
    // and a constructor to take individual values.
    Ray secondaryRay;

    secondaryRay.origin = psrResolverState.origin;
    secondaryRay.coneRadius = psrResolverState.coneRadius;
    secondaryRay.direction = psrResolverState.direction;
    secondaryRay.tMax = floatMax;

    // Trace the secondary ray
    // Todo: Find a perhaps less code-duplicatey way to do this later.

    EmptyExtraArgs extraArgs;
    // Note: Count back facing hits when inside a medium to not skip surfaces to exit transmission on.
    uint flags = psrResolverState.insideMedium ? 0 : RAY_FLAG_CULL_BACK_FACING_TRIANGLES;
    flags |= RAY_FLAG_FORCE_OPAQUE;

    RESOLVE_RAY_TRACE(
      extraArgs, secondaryRay, flags, GBUFFER_SBT_OFFSET_STANDARD,
      geometryPSRResolverVertex,
      cb.psrRayMaxInteractions, psrResolverState,
      psrResolverState.segmentHitDistance,
      DEBUG_VIEW_PRIMARY_RAY_INTERACTIONS, DEBUG_VIEW_PRIMARY_RAY_AND_UNORDERED_INTERACTIONS, true);
    
    // Terminate if PSR resolving should no longer continue

    if (!psrResolverState.continuePSRResolving)
    {
      break;
    }
  }

  // Add to the number of primary ray bounces in the debug view based on the number of PSR iterations

  if (cb.debugView == DEBUG_VIEW_PRIMARY_RAY_BOUNCES)
  {
    accumulateInDebugViewAdd(psrResolverState.pixelCoordinate, psrResolverState.bounceIteration);
  }
}

void geometryResolver(ivec2 pixelCoordinate) {
  // Initialize Geometry Resolver State

  // Note: Acts partially as live state due to some values being carried across both the
  // first hit resolve and the reflection PSR resolve logic to get to the transmission PSR
  // resolve logic.
  GeometryResolverState geometryResolverState = geometryResolverStateCreateEmpty(u16vec2(pixelCoordinate));

  if (cb.enableStochasticAlphaBlend)
  {
    RNG randomState = createRNG(geometryResolverState.pixelCoordinate, cb.frameIdx);
    AlphaBlendSurface alphaBlendSurface = AlphaBlendSurface.createEmpty();
    alphaBlendSurface.randomThreshold = getNextSampleBlueNoise(randomState);
    AlphaBlendGBuffer[pixelCoordinate] = alphaBlendSurface.pack();
  }
  ParticleBuffer[pixelCoordinate] = vec4(0,0,0,1);
  PrimaryAttenuation[pixelCoordinate] = colorToR11G11B10(geometryResolverState.attenuation);

#ifndef GBUFFER_PSR
  if (cb.enableObjectPicking != 0)
  {
    // In ray generation, clean up object picking buffer before writing
    PrimaryObjectPicking[pixelCoordinate] = OBJECT_PICKING_INVALID;
  }
#endif

  // Resolve First Hit Geometry
  {
    // Pass specific set up
    Ray primaryRay;
    preprocessPrimaryRayState(primaryRay, geometryResolverState);

    // If starting inside a medium, need to include back faces to detect an exit.
    uint flags = geometryResolverState.insideMedium ? 0 : RAY_FLAG_CULL_BACK_FACING_TRIANGLES;
    flags |= RAY_FLAG_FORCE_OPAQUE;

    // Raytrace

    EmptyExtraArgs extraArgs;

    RESOLVE_RAY_TRACE(
      extraArgs, primaryRay, flags, GBUFFER_SBT_OFFSET_STANDARD,
      geometryResolverVertex,
      cb.primaryRayMaxInteractions, geometryResolverState,
      geometryResolverState.segmentHitDistance,
      DEBUG_VIEW_PRIMARY_RAY_INTERACTIONS, DEBUG_VIEW_PRIMARY_RAY_AND_UNORDERED_INTERACTIONS, true);

    if (cb.debugView == DEBUG_VIEW_PRIMARY_RAY_BOUNCES)
    {
      accumulateInDebugViewAdd(geometryResolverState.pixelCoordinate, 1);
    }
  }

  // Translate PSR flags and the selected integration surface flag from reflection/transmission to a primary/secondary context

  bool primarySelectedIntegrationSurface;
  bool secondarySurfaceMask;

  if (geometryResolverState.performPSRR && geometryResolverState.performPSTR)
  {
    // Note: Transmission preferred as the primary surface when both PSRR and PSTR are done at the same time.
    primarySelectedIntegrationSurface = !geometryResolverState.reflectionSelectedIntegrationSurface;
    // Note: Secondary surface used for reflections when both PSRR and PSTR are active.
    secondarySurfaceMask = true;
  }
  else
  {
    // Note: Primary surface always the one to integrate off of if only PSRR, PSTR or neither is done.
    primarySelectedIntegrationSurface = true;
    // Note: Secondary surface never used if only one type of surface is present.
    secondarySurfaceMask = false;
  }

  // Output GBuffer flags
  // Note: Flags are always outputted so that other passes can read them without needing to know if a miss happened or not

  GeometryFlags geometryFlags;

  geometryFlags.primarySelectedIntegrationSurface = primarySelectedIntegrationSurface;
  geometryFlags.secondarySurfaceMask = secondarySurfaceMask;
  geometryFlags.isViewModel = geometryResolverState.isViewModelSurface;
  geometryFlags.portalSpace = geometryResolverState.portalSpace;
  geometryFlags.objectMask = primaryRayMaskToObjectMask(geometryResolverState.rayMask, geometryResolverState.isViewModelSurface);
  // Note: Set by direct lighting integrator later, hardcoded to false here.
  geometryFlags.insideMedium = false;
  geometryFlags.firstSampledLobeIsSpecular = false;
  geometryFlags.performPSTR = geometryResolverState.performPSTR;
  geometryFlags.performPSRR = geometryResolverState.performPSRR;
  geometryFlags.pomOpaqueSurfaceEncountered = geometryResolverState.pomOpaqueSurfaceEncountered;

  // Output miss data for the secondary surface if it is not present
  // Note: Known to not be the selected surface in this case, hence why false is passed for this.

  if (!secondarySurfaceMask)
  {
    geometryResolverOutputMiss(false, false, geometryResolverState.pixelCoordinate);
  }

  // Write the geometry flags

  geometryFlagsWriteToGBuffer(geometryFlags, ivec2(pixelCoordinate), SharedFlags);

  // Output to debug view
  geometryResolverOutputDebugView(pixelCoordinate);
}

void geometryPSRResolverPass(ivec2 pixelCoordinate) {
  GeometryFlags geometryFlags = geometryFlagsReadFromGBuffer(pixelCoordinate, SharedFlags);

  // Early out if the PSR pass currently executing does not handle a requested PSR type

  /*
  if (!push.isTransmissionPSR && !geometryFlags.performPSRR)
  {
    return;
  }
  else if (push.isTransmissionPSR && !geometryFlags.performPSTR)
  {
    return;
  }
  */

  // Hack: This code is the same as the code commented out above, but is done this way to trick the Intel shader compiler
  // into avoiding a bug which causes this conditional to behave incorrectly. See REMIX-1637 for more info or this issue:
  // https://github.com/IGCIT/Intel-GPU-Community-Issue-Tracker-IGCIT/issues/318
  if (
    uint(!push.isTransmissionPSR) * uint(!geometryFlags.performPSRR) +
    uint(push.isTransmissionPSR) * uint(!geometryFlags.performPSTR)
  )
  {
    return;
  }

  // Re-load the Geometry PSR Resolver state from memory

  GeometryPSRResolverState geometryPSRResolverState;

  geometryPSRResolverState.bounceIteration = 0;
  geometryPSRResolverState.coneRadius = PrimaryConeRadius[pixelCoordinate];
  geometryPSRResolverState.segmentHitDistance = 0;
  geometryPSRResolverState.segmentHitDistance = 0.0f;
  geometryPSRResolverState.continueResolving = true;
  geometryPSRResolverState.continuePSRResolving = true;
  geometryPSRResolverState.isViewModelSurface = false;
  geometryPSRResolverState.decalEncountered = false;
  geometryPSRResolverState.isTransmissionPSR = push.isTransmissionPSR;
  geometryPSRResolverState.pomOpaqueSurfaceEncountered = false;

  GbufferPSRData psrData;
  if (push.isTransmissionPSR)
  {
    psrData = loadTransmissionPSRData(pixelCoordinate);
  }
  else
  {
    psrData = loadReflectionPSRData(pixelCoordinate);
  }

  unpackPSRData(psrData, geometryPSRResolverState);

  if (!geometryPSRResolverState.isValid)
  {
    return;
  }
  
  if (push.isTransmissionPSR)
  {
    // Note: performPSTR == true implicitly in this path, always indicate that transmission PSR is the primary surface
    // when it is in use.
    geometryPSRResolverState.primarySurface = true;
    
    // Don't show the player model in transmission PSR unless it's also shown in primary space.
    // Sometimes, the invisible player model clips through a window, and the gun sticking through a window looks wrong.
    if (!cb.enablePlayerModelInPrimarySpace && geometryPSRResolverState.portalSpace == PORTAL_SPACE_NONE)
      geometryPSRResolverState.rayMask &= ~OBJECT_MASK_PLAYER_MODEL;
  }
  else
  {
    // Note: performPSRR == true implicitly in this path due to it being checked before this function is called,
    // therefore reflection PSR is the primary surface in all cases other than if PSTR is being used. This is done
    // to allow for the transmission side of glass to have higher quality as the primary surface is allowed to use
    // RTXDI.
    geometryPSRResolverState.primarySurface = !geometryFlags.performPSTR;

    // Always show the player model in reflections
    geometryPSRResolverState.rayMask |= OBJECT_MASK_ALL_PLAYER_MODEL;
  }

  if (geometryPSRResolverState.primarySurface)
  {
    geometryPSRResolverState.accumulatedHitDistance = PrimaryHitDistance[pixelCoordinate];
    geometryPSRResolverState.useAlternateDisocclusionThreshold = PrimaryDisocclusionThresholdMix[pixelCoordinate] != 0;
    geometryPSRResolverState.selectedIntegrationSurface = geometryFlags.primarySelectedIntegrationSurface;
  }
  else
  {
    geometryPSRResolverState.accumulatedHitDistance = SecondaryHitDistance[pixelCoordinate];
    geometryPSRResolverState.useAlternateDisocclusionThreshold = false; // doesn't matter
    geometryPSRResolverState.selectedIntegrationSurface = !geometryFlags.primarySelectedIntegrationSurface;
  }

  geometryPSRResolverState.radiance = vec3(SharedRadianceRG[pixelCoordinate], SharedRadianceB[pixelCoordinate]);

  // Invoke the Geometry PSR Resolver
  
  geometryPSRResolver(geometryPSRResolverState);

  // If this is the surface that will be used for integration, remember its view model flag and portal space.
  
  if (geometryPSRResolverState.selectedIntegrationSurface)
  {
    // Rematerialize geometryFlags to reduce live state in the resolve loop
    geometryFlags = geometryFlagsReadFromGBuffer(pixelCoordinate, SharedFlags);

    // Update the flags and store
    geometryFlags.isViewModel = geometryPSRResolverState.isViewModelSurface;
    geometryFlags.portalSpace = geometryPSRResolverState.portalSpace;
    geometryFlags.pomOpaqueSurfaceEncountered = geometryPSRResolverState.pomOpaqueSurfaceEncountered;
    geometryFlags.objectMask = primaryRayMaskToObjectMask(geometryPSRResolverState.rayMask, geometryPSRResolverState.isViewModelSurface);

    geometryFlagsWriteToGBuffer(geometryFlags, pixelCoordinate, SharedFlags);
  }
}
